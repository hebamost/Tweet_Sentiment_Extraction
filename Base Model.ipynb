{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing and Importing Required Libraries . ","metadata":{}},{"cell_type":"code","source":"!pip install contractions\n!pip install pyspellchecker\n!pip install -U textblob","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:22:49.757084Z","iopub.execute_input":"2023-09-14T01:22:49.757756Z","iopub.status.idle":"2023-09-14T01:23:28.450066Z","shell.execute_reply.started":"2023-09-14T01:22:49.757728Z","shell.execute_reply":"2023-09-14T01:23:28.448854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m textblob.download_corpora","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:23:31.298780Z","iopub.execute_input":"2023-09-14T01:23:31.299300Z","iopub.status.idle":"2023-09-14T01:23:34.569862Z","shell.execute_reply.started":"2023-09-14T01:23:31.299263Z","shell.execute_reply":"2023-09-14T01:23:34.568704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:23:36.725517Z","iopub.execute_input":"2023-09-14T01:23:36.725904Z","iopub.status.idle":"2023-09-14T01:23:48.403165Z","shell.execute_reply.started":"2023-09-14T01:23:36.725858Z","shell.execute_reply":"2023-09-14T01:23:48.401845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:23:51.186044Z","iopub.execute_input":"2023-09-14T01:23:51.186404Z","iopub.status.idle":"2023-09-14T01:23:52.449739Z","shell.execute_reply.started":"2023-09-14T01:23:51.186375Z","shell.execute_reply":"2023-09-14T01:23:52.448641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk \nimport string\nimport string\nimport contractions\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\ntqdm_notebook.pandas()\nfrom sklearn.model_selection import train_test_split\nimport random\nimport os\nimport sys\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-14T01:28:36.762223Z","iopub.execute_input":"2023-09-14T01:28:36.762584Z","iopub.status.idle":"2023-09-14T01:28:36.770700Z","shell.execute_reply.started":"2023-09-14T01:28:36.762557Z","shell.execute_reply":"2023-09-14T01:28:36.769568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, one_hot\nfrom keras.utils import pad_sequences\nfrom more_itertools import locate","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:13.071622Z","iopub.execute_input":"2023-09-14T01:24:13.072525Z","iopub.status.idle":"2023-09-14T01:24:13.123675Z","shell.execute_reply.started":"2023-09-14T01:24:13.072474Z","shell.execute_reply":"2023-09-14T01:24:13.122706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding,Dense,Dropout,Concatenate,Flatten,Input,GRU,BatchNormalization,Bidirectional,SpatialDropout1D,LSTM,LayerNormalization\n\nfrom tensorflow.keras import Model, regularizers, optimizers\nfrom tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:14.746338Z","iopub.execute_input":"2023-09-14T01:24:14.746703Z","iopub.status.idle":"2023-09-14T01:24:14.756351Z","shell.execute_reply.started":"2023-09-14T01:24:14.746675Z","shell.execute_reply":"2023-09-14T01:24:14.755377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data from kaggle Input files","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:19.434230Z","iopub.execute_input":"2023-09-14T01:24:19.434595Z","iopub.status.idle":"2023-09-14T01:24:19.461422Z","shell.execute_reply.started":"2023-09-14T01:24:19.434566Z","shell.execute_reply":"2023-09-14T01:24:19.460445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('/kaggle/input')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:22.047888Z","iopub.execute_input":"2023-09-14T01:24:22.048267Z","iopub.status.idle":"2023-09-14T01:24:22.053031Z","shell.execute_reply.started":"2023-09-14T01:24:22.048237Z","shell.execute_reply":"2023-09-14T01:24:22.051937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:24.661883Z","iopub.execute_input":"2023-09-14T01:24:24.662282Z","iopub.status.idle":"2023-09-14T01:24:24.679503Z","shell.execute_reply.started":"2023-09-14T01:24:24.662252Z","shell.execute_reply":"2023-09-14T01:24:24.678401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the data to a pandas dataframe\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest  = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nabbreviations = pd.read_csv(\"/kaggle/input/chat-slang-abbreviations-acronyms/slang/slang.csv\")\nabrevtn_dict   = dict(zip(abbreviations.acronym, abbreviations.expansion))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:27.143746Z","iopub.execute_input":"2023-09-14T01:24:27.144462Z","iopub.status.idle":"2023-09-14T01:24:27.332532Z","shell.execute_reply.started":"2023-09-14T01:24:27.144425Z","shell.execute_reply":"2023-09-14T01:24:27.331546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:28.849693Z","iopub.execute_input":"2023-09-14T01:24:28.850404Z","iopub.status.idle":"2023-09-14T01:24:28.872289Z","shell.execute_reply.started":"2023-09-14T01:24:28.850367Z","shell.execute_reply":"2023-09-14T01:24:28.871410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:30.674650Z","iopub.execute_input":"2023-09-14T01:24:30.675045Z","iopub.status.idle":"2023-09-14T01:24:30.758582Z","shell.execute_reply.started":"2023-09-14T01:24:30.675009Z","shell.execute_reply":"2023-09-14T01:24:30.757506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:31.586689Z","iopub.execute_input":"2023-09-14T01:24:31.587439Z","iopub.status.idle":"2023-09-14T01:24:31.593936Z","shell.execute_reply.started":"2023-09-14T01:24:31.587403Z","shell.execute_reply":"2023-09-14T01:24:31.592833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:32.269683Z","iopub.execute_input":"2023-09-14T01:24:32.270393Z","iopub.status.idle":"2023-09-14T01:24:32.276546Z","shell.execute_reply.started":"2023-09-14T01:24:32.270358Z","shell.execute_reply":"2023-09-14T01:24:32.275561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:32.670492Z","iopub.execute_input":"2023-09-14T01:24:32.671111Z","iopub.status.idle":"2023-09-14T01:24:32.685088Z","shell.execute_reply.started":"2023-09-14T01:24:32.671081Z","shell.execute_reply":"2023-09-14T01:24:32.683870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:33.152107Z","iopub.execute_input":"2023-09-14T01:24:33.152412Z","iopub.status.idle":"2023-09-14T01:24:33.162361Z","shell.execute_reply.started":"2023-09-14T01:24:33.152387Z","shell.execute_reply":"2023-09-14T01:24:33.161072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values\n\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:33.550291Z","iopub.execute_input":"2023-09-14T01:24:33.550591Z","iopub.status.idle":"2023-09-14T01:24:33.590708Z","shell.execute_reply.started":"2023-09-14T01:24:33.550566Z","shell.execute_reply":"2023-09-14T01:24:33.589557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:34.146570Z","iopub.execute_input":"2023-09-14T01:24:34.146881Z","iopub.status.idle":"2023-09-14T01:24:34.187020Z","shell.execute_reply.started":"2023-09-14T01:24:34.146855Z","shell.execute_reply":"2023-09-14T01:24:34.186016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:34.732313Z","iopub.execute_input":"2023-09-14T01:24:34.732637Z","iopub.status.idle":"2023-09-14T01:24:34.771157Z","shell.execute_reply.started":"2023-09-14T01:24:34.732611Z","shell.execute_reply":"2023-09-14T01:24:34.770113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values\n\ntest.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:35.097440Z","iopub.execute_input":"2023-09-14T01:24:35.097735Z","iopub.status.idle":"2023-09-14T01:24:35.110098Z","shell.execute_reply.started":"2023-09-14T01:24:35.097709Z","shell.execute_reply":"2023-09-14T01:24:35.109107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:35.519332Z","iopub.execute_input":"2023-09-14T01:24:35.521665Z","iopub.status.idle":"2023-09-14T01:24:35.531541Z","shell.execute_reply.started":"2023-09-14T01:24:35.521635Z","shell.execute_reply":"2023-09-14T01:24:35.530455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values\ntest.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:36.136740Z","iopub.execute_input":"2023-09-14T01:24:36.137479Z","iopub.status.idle":"2023-09-14T01:24:36.149014Z","shell.execute_reply.started":"2023-09-14T01:24:36.137445Z","shell.execute_reply":"2023-09-14T01:24:36.147866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Dataset","metadata":{}},{"cell_type":"code","source":"sentences = train[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:39.056290Z","iopub.execute_input":"2023-09-14T01:24:39.057523Z","iopub.status.idle":"2023-09-14T01:24:39.063352Z","shell.execute_reply.started":"2023-09-14T01:24:39.057471Z","shell.execute_reply":"2023-09-14T01:24:39.062134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences2 = \" \".join(sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:40.416509Z","iopub.execute_input":"2023-09-14T01:24:40.416897Z","iopub.status.idle":"2023-09-14T01:24:40.423201Z","shell.execute_reply.started":"2023-09-14T01:24:40.416866Z","shell.execute_reply":"2023-09-14T01:24:40.422292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To show most frequentsd words in our data\n\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences2))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:24:42.994345Z","iopub.execute_input":"2023-09-14T01:24:42.995250Z","iopub.status.idle":"2023-09-14T01:24:45.574537Z","shell.execute_reply.started":"2023-09-14T01:24:42.995206Z","shell.execute_reply":"2023-09-14T01:24:45.573627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_text(text):\n  text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n  text=re.sub('`',\"'\",text)        #Replacing apostrophe\n  text=re.sub('\\S*\\d\\S*',' ',text) #Removing Numbers\n  text=re.sub('<.*?>+',' ',text)   #Removing Angular Brackets\n  text=re.sub('\\[.*?\\]',' ',text)  #Removing Square Brackets\n  text=re.sub('\\n',' ',text)       #Removing '\\n' character \n  text=re.sub('\\*+','INSULT',text) #Replacing **** by INSULT\n  return text","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:01.336548Z","iopub.execute_input":"2023-09-14T01:25:01.336942Z","iopub.status.idle":"2023-09-14T01:25:01.343444Z","shell.execute_reply.started":"2023-09-14T01:25:01.336886Z","shell.execute_reply":"2023-09-14T01:25:01.342420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(clear_text)\n\ntrain['text']","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:04.521909Z","iopub.execute_input":"2023-09-14T01:25:04.522286Z","iopub.status.idle":"2023-09-14T01:25:05.449803Z","shell.execute_reply.started":"2023-09-14T01:25:04.522254Z","shell.execute_reply":"2023-09-14T01:25:05.448848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive = train[train[\"sentiment\"] == \"positive\"]\npositive","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:06.341595Z","iopub.execute_input":"2023-09-14T01:25:06.341952Z","iopub.status.idle":"2023-09-14T01:25:06.364921Z","shell.execute_reply.started":"2023-09-14T01:25:06.341924Z","shell.execute_reply":"2023-09-14T01:25:06.363710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative = train[train[\"sentiment\"] == \"negative\"]\nnegative","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:07.164798Z","iopub.execute_input":"2023-09-14T01:25:07.165520Z","iopub.status.idle":"2023-09-14T01:25:07.187837Z","shell.execute_reply.started":"2023-09-14T01:25:07.165485Z","shell.execute_reply":"2023-09-14T01:25:07.183968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neutral = train[train[\"sentiment\"] == \"neutral\"]\nneutral","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:07.769400Z","iopub.execute_input":"2023-09-14T01:25:07.769717Z","iopub.status.idle":"2023-09-14T01:25:07.791757Z","shell.execute_reply.started":"2023-09-14T01:25:07.769688Z","shell.execute_reply":"2023-09-14T01:25:07.790794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To show most frequentsd words in positive data samples\npos = positive[\"text\"].tolist()\npos2 = \" \".join(pos)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(pos2))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:08.374098Z","iopub.execute_input":"2023-09-14T01:25:08.374723Z","iopub.status.idle":"2023-09-14T01:25:09.588713Z","shell.execute_reply.started":"2023-09-14T01:25:08.374693Z","shell.execute_reply":"2023-09-14T01:25:09.587837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To show most frequentsd words in negative data samples\nneg = negative[\"text\"].tolist()\nneg2 = \" \".join(neg)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(neg2))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:10.608455Z","iopub.execute_input":"2023-09-14T01:25:10.609140Z","iopub.status.idle":"2023-09-14T01:25:11.763408Z","shell.execute_reply.started":"2023-09-14T01:25:10.609105Z","shell.execute_reply":"2023-09-14T01:25:11.762245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"port_stem = PorterStemmer()\nspell = SpellChecker()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:13.850466Z","iopub.execute_input":"2023-09-14T01:25:13.851509Z","iopub.status.idle":"2023-09-14T01:25:13.928381Z","shell.execute_reply.started":"2023-09-14T01:25:13.851463Z","shell.execute_reply":"2023-09-14T01:25:13.927411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_hyperlinks(text):\n  hyperlinkfree = re.sub('https?://\\S+|www\\.\\S+', '', text)\n  return hyperlinkfree","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:15.848412Z","iopub.execute_input":"2023-09-14T01:25:15.849248Z","iopub.status.idle":"2023-09-14T01:25:15.854884Z","shell.execute_reply.started":"2023-09-14T01:25:15.849211Z","shell.execute_reply":"2023-09-14T01:25:15.853920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(text):\n  \n  stemmed_content = text.lower()\n  stemmed_content = stemmed_content.split()\n  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n  stemmed_content = ' '.join(stemmed_content)\n  return stemmed_content\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:18.886323Z","iopub.execute_input":"2023-09-14T01:25:18.886695Z","iopub.status.idle":"2023-09-14T01:25:18.892945Z","shell.execute_reply.started":"2023-09-14T01:25:18.886666Z","shell.execute_reply":"2023-09-14T01:25:18.891787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_text(text):\n  text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n  text=re.sub('`',\"'\",text)        #Replacing apostrophe\n  text=re.sub('\\S*\\d\\S*',' ',text) #Removing Numbers\n  text=re.sub('<.*?>+',' ',text)   #Removing Angular Brackets\n  text=re.sub('\\[.*?\\]',' ',text)  #Removing Square Brackets\n  text=re.sub('\\n',' ',text)       #Removing '\\n' character \n  text=re.sub('\\*+','INSULT',text) #Replacing **** by INSULT\n  return text","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:20.134238Z","iopub.execute_input":"2023-09-14T01:25:20.134600Z","iopub.status.idle":"2023-09-14T01:25:20.141117Z","shell.execute_reply.started":"2023-09-14T01:25:20.134571Z","shell.execute_reply":"2023-09-14T01:25:20.140156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_contractions(text):\n    expanded_words = []   \n    for word in text.split():\n      # using contractions.fix to expand the shortened words\n      expanded_words.append(contractions.fix(word))     \n    expanded_text = contractions.fix(text)\n    return expanded_text\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:21.606135Z","iopub.execute_input":"2023-09-14T01:25:21.606862Z","iopub.status.idle":"2023-09-14T01:25:21.612626Z","shell.execute_reply.started":"2023-09-14T01:25:21.606825Z","shell.execute_reply":"2023-09-14T01:25:21.611301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def abbrev_word(word):\n    if word in abrevtn_dict.keys():\n        return abrevtn_dict[word]\n    else: \n        return word\n    \ndef abbrev2_text(text):\n    sentnce = [abbrev_word(word) for word in text.split()]\n    text = ' '.join(sentnce)\n    return text\n ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:25:22.705335Z","iopub.execute_input":"2023-09-14T01:25:22.705692Z","iopub.status.idle":"2023-09-14T01:25:22.711441Z","shell.execute_reply.started":"2023-09-14T01:25:22.705663Z","shell.execute_reply":"2023-09-14T01:25:22.710264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to remove links from our data as it wouldn't affect our prediction or the meaning of the sentence \n\ntrain['text'] = train['text'].apply(remove_hyperlinks)\nprint(train['text'])\n\ntrain['selected_text'] = train['selected_text'].apply(remove_hyperlinks)\nprint(\"\\n\\n\", train['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:07.803445Z","iopub.execute_input":"2023-09-14T01:26:07.804027Z","iopub.status.idle":"2023-09-14T01:26:07.998301Z","shell.execute_reply.started":"2023-09-14T01:26:07.803986Z","shell.execute_reply":"2023-09-14T01:26:07.997310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to expand apprevations from the data ex : \"I'd ve\" would be replaced by \"I would have\"\n\ntrain['text'] = train['text'].apply(expand_contractions)\nprint(train['text'])\n\ntrain['selected_text'] = train['selected_text'].apply(expand_contractions)\nprint(\"\\n\\n\",train['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:26.744753Z","iopub.execute_input":"2023-09-14T01:26:26.745660Z","iopub.status.idle":"2023-09-14T01:26:29.734438Z","shell.execute_reply.started":"2023-09-14T01:26:26.745613Z","shell.execute_reply":"2023-09-14T01:26:29.733348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to make the text clear by replacing apostrophe, numbers , etc.... which would make understanding words by model more easier\n\ntrain['text'] = train['text'].apply(clear_text)\nprint(train['text'])\n\ntrain['selected_text'] = train['selected_text'].apply(clear_text)\nprint(\"\\n\\n\",train['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:35.400624Z","iopub.execute_input":"2023-09-14T01:26:35.401006Z","iopub.status.idle":"2023-09-14T01:26:36.959470Z","shell.execute_reply.started":"2023-09-14T01:26:35.400976Z","shell.execute_reply":"2023-09-14T01:26:36.958351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To expand abbrevations like \"OMG\" it became \"Oh My God\"\n\ntrain['text'] = train['text'].apply(abbrev2_text)\nprint(train['text'])\n\ntrain['selected_text'] = train['selected_text'].apply(abbrev2_text)\nprint(\"\\n\\n\",train['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:42.409369Z","iopub.execute_input":"2023-09-14T01:26:42.409797Z","iopub.status.idle":"2023-09-14T01:26:42.772120Z","shell.execute_reply.started":"2023-09-14T01:26:42.409761Z","shell.execute_reply":"2023-09-14T01:26:42.770387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatizer ","metadata":{}},{"cell_type":"code","source":"print(train['text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:47.333619Z","iopub.execute_input":"2023-09-14T01:26:47.334001Z","iopub.status.idle":"2023-09-14T01:26:47.341643Z","shell.execute_reply.started":"2023-09-14T01:26:47.333971Z","shell.execute_reply":"2023-09-14T01:26:47.340582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regular_punct = list(string.punctuation)\n\ndef spacing_punctuation(text):\n    # Add spaces before and after punctuations\n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:49.525359Z","iopub.execute_input":"2023-09-14T01:26:49.526202Z","iopub.status.idle":"2023-09-14T01:26:49.532104Z","shell.execute_reply.started":"2023-09-14T01:26:49.526168Z","shell.execute_reply":"2023-09-14T01:26:49.531009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to space punctuation which helps in tokenization .\n\ntrain['text'] = train['text'].apply(spacing_punctuation)\nprint(train['text'])\n\ntrain['selected_text'] = train['selected_text'].apply(spacing_punctuation)\nprint(\"\\n\\n\", train['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:26:58.775789Z","iopub.execute_input":"2023-09-14T01:26:58.776483Z","iopub.status.idle":"2023-09-14T01:26:59.099816Z","shell.execute_reply.started":"2023-09-14T01:26:58.776449Z","shell.execute_reply":"2023-09-14T01:26:59.098793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To make sure special characters are handled in word embedding matrix","metadata":{}},{"cell_type":"code","source":"# loading the words embedding from glove dictionary to use it later in the model embedding layer\n\nembeddings_index = {}\npath = \"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\"\n\nf = open(path)\nfor line in f:\n    values = line.split(' ')\n    # The first element indicate the word\n    word = values[0]      \n    # And rest of the part is vector representation of that word.\n    coefs = np.asarray(values[1:], dtype='float32') \n    embeddings_index[word] = coefs\nf.close()\n\nprint('The number of words in GloVe : ', len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:27:01.722434Z","iopub.execute_input":"2023-09-14T01:27:01.722801Z","iopub.status.idle":"2023-09-14T01:27:14.408262Z","shell.execute_reply.started":"2023-09-14T01:27:01.722772Z","shell.execute_reply":"2023-09-14T01:27:14.407267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regular_punct = list(string.punctuation)\nregular_punct","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:28:50.548792Z","iopub.execute_input":"2023-09-14T01:28:50.549185Z","iopub.status.idle":"2023-09-14T01:28:50.556203Z","shell.execute_reply.started":"2023-09-14T01:28:50.549154Z","shell.execute_reply":"2023-09-14T01:28:50.555186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cjecking which symbols aren't supported in our embedding dictionary \n\nall_punct = list(set(regular_punct))\nnot_supported_symbol = []\nfor e in all_punct:\n    if e not in embeddings_index:\n        not_supported_symbol.append(e)\nprint(\"Special symbol not present in word embedding : \", not_supported_symbol)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:30:17.955564Z","iopub.execute_input":"2023-09-14T01:30:17.955963Z","iopub.status.idle":"2023-09-14T01:30:17.961946Z","shell.execute_reply.started":"2023-09-14T01:30:17.955928Z","shell.execute_reply":"2023-09-14T01:30:17.960942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['text'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:30:19.036460Z","iopub.execute_input":"2023-09-14T01:30:19.037231Z","iopub.status.idle":"2023-09-14T01:30:19.044185Z","shell.execute_reply.started":"2023-09-14T01:30:19.037191Z","shell.execute_reply":"2023-09-14T01:30:19.042829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting selected data to its corresponding indices in the original text","metadata":{}},{"cell_type":"code","source":"# here we search for the index of the first word of the selected text in the original text and store that index \n# as start index in indices array touse it in the evaluation process\n\ndef find_start_indices(x):\n    text,sel_text  = x[0],x[1]\n    text = text.split()\n    sel_text = sel_text.split()\n    \n    if len(sel_text) == 0 or len(text) == 0:\n        return 0\n    \n    end = sel_text[0]\n    if end in text:\n        index = text.index(end)\n        return index\n    else:\n        return 0\n\n    \n    \ntrain['start_indices'] = train[['text','selected_text']].apply(lambda x:find_start_indices(x),axis=1)\n\n\n# here we search for the index of the last word of the selected text in the original text and store that index \n# as the end index in indices array touse it in the evaluation process\n\ndef find_end_indices(x):\n    text,sel_text,start_indices  = x[0],x[1],x[2]\n    text = text.split()\n    sel_text = sel_text.split()\n    \n    if len(sel_text) == 0 or len(text) == 0:\n        return start_indices\n    \n    end = sel_text[-1]\n    try:\n        index = text.index(end,start_indices)\n    except:\n        if end in text:\n            index = text.index(end)\n            return index\n        else: return start_indices + 1\n    return index\n\ntrain['end_indices'] = train[['text','selected_text','start_indices']].progress_apply(lambda x:find_end_indices(x),axis=1)\n\n\ntrain.sample(15)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:34:49.639987Z","iopub.execute_input":"2023-09-14T01:34:49.641152Z","iopub.status.idle":"2023-09-14T01:34:50.704641Z","shell.execute_reply.started":"2023-09-14T01:34:49.641103Z","shell.execute_reply":"2023-09-14T01:34:50.703438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to return the indices of the sentences with length >= 30 to drop them\n\ndef drop_long_sentences(text_list):\n    indices = locate(text_list, lambda x: len(x) > 30)\n    return list(indices)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:36:09.422728Z","iopub.execute_input":"2023-09-14T01:36:09.423120Z","iopub.status.idle":"2023-09-14T01:36:09.428152Z","shell.execute_reply.started":"2023-09-14T01:36:09.423090Z","shell.execute_reply":"2023-09-14T01:36:09.427076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to return the indices of the sentences with certain sentiment to drop them\n\ndef drop_sentiment(text_list, sentiment):\n    indices = locate(text_list, lambda x: x == sentiment)\n    return list(indices)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:36:10.636346Z","iopub.execute_input":"2023-09-14T01:36:10.636718Z","iopub.status.idle":"2023-09-14T01:36:10.641614Z","shell.execute_reply.started":"2023-09-14T01:36:10.636687Z","shell.execute_reply":"2023-09-14T01:36:10.640677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"total number of training samples before dropping >> \", train.shape[0], \"\\n\")\ntrain_txt = train['text'].values\ntokenizer_text = Tokenizer(lower = True,split = ' ',oov_token = 'oov')\ntokenizer_text.fit_on_texts(train_txt)\ntrain_txt = tokenizer_text.texts_to_sequences(train_txt)\nmax_before = max(len(seq) for seq in train_txt)\nindx = drop_long_sentences(train_txt)\ntext_len_list = [len(seq) for seq in train_txt]\ntrain.drop(train.index[indx], inplace = True)\ntrain_txt = np.delete(train_txt, indx).tolist()\nmax_text_len = max(len(seq) for seq in train_txt)\nprint(\"total number of training samples after dropping >>\", len(train_txt), \"\\n\")\nprint(\"max length before dropping >> \", max_before, \"\\n\")\nprint(\"max length after dropping >> \", max_text_len, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:36:18.330391Z","iopub.execute_input":"2023-09-14T01:36:18.330756Z","iopub.status.idle":"2023-09-14T01:36:19.504865Z","shell.execute_reply.started":"2023-09-14T01:36:18.330725Z","shell.execute_reply":"2023-09-14T01:36:19.503937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_len, freq = np.unique(text_len_list, return_counts=True)\nprint(unique_len, \"\\n\")\nprint(freq, \"\\n\")\nfig, ax = plt.subplots(figsize =(10, 8))\nax.barh(unique_len, freq, color ='maroon')\nax.invert_yaxis()\nplt.title(\"length distribution for training sentences\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:36:53.537066Z","iopub.execute_input":"2023-09-14T01:36:53.537462Z","iopub.status.idle":"2023-09-14T01:36:53.906589Z","shell.execute_reply.started":"2023-09-14T01:36:53.537433Z","shell.execute_reply":"2023-09-14T01:36:53.905658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = [len(train_txt), len(indx)]\nlab = [\"used data for training\", \"dropped data\"]\nplt.pie(val, labels = lab, autopct='%1.2f%%')\nplt.legend()\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:36:55.036819Z","iopub.execute_input":"2023-09-14T01:36:55.037202Z","iopub.status.idle":"2023-09-14T01:36:55.258761Z","shell.execute_reply.started":"2023-09-14T01:36:55.037172Z","shell.execute_reply":"2023-09-14T01:36:55.257338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This section is only to test the model performance on positive and negative data only so it would drop the neutral samples from our data","metadata":{}},{"cell_type":"code","source":"\n#train_sent = train['sentiment'].values\n#ind = drop_sentiment(train_sent, \"neutral\")\n#print(len(ind))\n#random.shuffle(ind)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:31:16.641376Z","iopub.execute_input":"2023-09-09T13:31:16.641747Z","iopub.status.idle":"2023-09-09T13:31:16.663806Z","shell.execute_reply.started":"2023-09-09T13:31:16.641719Z","shell.execute_reply":"2023-09-09T13:31:16.662762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.drop(train.index[ind], inplace=True)\n#print(train.shape[0])\n#train","metadata":{"execution":{"iopub.status.busy":"2023-09-08T18:39:59.999103Z","iopub.execute_input":"2023-09-08T18:39:59.999524Z","iopub.status.idle":"2023-09-08T18:40:00.023672Z","shell.execute_reply.started":"2023-09-08T18:39:59.999492Z","shell.execute_reply":"2023-09-08T18:40:00.022702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.drop(train.index[ind[:3000]], inplace=True)\n#print(train.shape[0])\n#train","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:31:25.055632Z","iopub.execute_input":"2023-09-09T13:31:25.056003Z","iopub.status.idle":"2023-09-09T13:31:25.079741Z","shell.execute_reply.started":"2023-09-09T13:31:25.055973Z","shell.execute_reply":"2023-09-09T13:31:25.078247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:16:49.140688Z","iopub.execute_input":"2023-09-13T01:16:49.141709Z","iopub.status.idle":"2023-09-13T01:16:49.156798Z","shell.execute_reply.started":"2023-09-13T01:16:49.141659Z","shell.execute_reply":"2023-09-13T01:16:49.155853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Training Dataset","metadata":{}},{"cell_type":"code","source":"train_labels = train[['start_indices', 'end_indices']]\ntrain_data = train[['text', 'sentiment']]\ntrain_text_data, analysis_data, train_label_data, analysis_label = train_test_split(train_data, train_labels, test_size = 0.2, random_state = 42, shuffle = True)\n\ntrain_text_data, validation_text, train_label_data, validation_label = train_test_split(train_text_data, train_label_data, test_size = 0.06, random_state = 42, shuffle = True)\n\nprint(\"\\n training dataset size\", len(train_text_data))\nprint(\"\\n validation dataset size\",len(validation_text))\nprint(\"\\n dataset used for analysis size\",len(analysis_data))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:39:42.301724Z","iopub.execute_input":"2023-09-14T01:39:42.302319Z","iopub.status.idle":"2023-09-14T01:39:42.331948Z","shell.execute_reply.started":"2023-09-14T01:39:42.302276Z","shell.execute_reply":"2023-09-14T01:39:42.331078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Training and validation data with padding to pass it as inputs to the model","metadata":{}},{"cell_type":"code","source":"train_text = train_text_data['text'].values\nval_text = validation_text['text'].values\n\ntokenizer_text = Tokenizer(lower = True, split = ' ', oov_token = 'oov')\ntokenizer_text.fit_on_texts(train_text)\n\n\ntrain_text = tokenizer_text.texts_to_sequences(train_text)\nval_text = tokenizer_text.texts_to_sequences(val_text)\n\nvocab_size_text = len(tokenizer_text.word_index) + 1\n\nword_index_text = tokenizer_text.word_index\n\ntrain_text = pad_sequences(train_text, maxlen = max_text_len, padding = 'post')\nval_text = pad_sequences(val_text, maxlen = max_text_len, padding = 'post')\n\nprint(train_text.shape,val_text.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:41:27.454091Z","iopub.execute_input":"2023-09-14T01:41:27.454469Z","iopub.status.idle":"2023-09-14T01:41:28.345715Z","shell.execute_reply.started":"2023-09-14T01:41:27.454440Z","shell.execute_reply":"2023-09-14T01:41:28.344631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentiment = train_text_data['sentiment'].values\nval_sentiment = validation_text['sentiment'].values\n\ntokenizer_sentiment = Tokenizer(lower = True, split = ' ', oov_token = 'oov')\ntokenizer_sentiment.fit_on_texts(train_sentiment)\n\ntrain_sentiment = tokenizer_sentiment.texts_to_sequences(train_sentiment)\nval_sentiment = tokenizer_sentiment.texts_to_sequences(val_sentiment)\n\nmax_sentiment_len = max(len(seq) for seq in train_sentiment)\n\n\n\ntrain_sentiment = pad_sequences(train_sentiment, maxlen = max_sentiment_len, padding = 'post')\nval_sentiment = pad_sequences(val_sentiment, maxlen = max_sentiment_len, padding = 'post')\n\nvocab_size_sentiment = len(tokenizer_sentiment.word_index) + 1\nword_index_sentiment = tokenizer_sentiment.word_index\n\nprint(train_sentiment.shape,val_sentiment.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:41:30.961554Z","iopub.execute_input":"2023-09-14T01:41:30.961930Z","iopub.status.idle":"2023-09-14T01:41:31.385249Z","shell.execute_reply.started":"2023-09-14T01:41:30.961879Z","shell.execute_reply":"2023-09-14T01:41:31.384083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_inp = (val_text, val_sentiment)\nval_out = validation_label.values\nval_data = (val_inp, val_out)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:45:43.493704Z","iopub.execute_input":"2023-09-14T01:45:43.494140Z","iopub.status.idle":"2023-09-14T01:45:43.499963Z","shell.execute_reply.started":"2023-09-14T01:45:43.494104Z","shell.execute_reply":"2023-09-14T01:45:43.498941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Embedding Matrix for text and sentiment","metadata":{}},{"cell_type":"code","source":"# create embedding index\nembedding_index = {}\nwith open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:42:45.286928Z","iopub.execute_input":"2023-09-14T01:42:45.287417Z","iopub.status.idle":"2023-09-14T01:42:56.583829Z","shell.execute_reply.started":"2023-09-14T01:42:45.287378Z","shell.execute_reply":"2023-09-14T01:42:56.582763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix_text=np.zeros((vocab_size_text, 100))\nfor word, i in word_index_text.items():\n    embedding_vector=embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_text[i]=embedding_vector\n ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:00.283702Z","iopub.execute_input":"2023-09-14T01:43:00.284070Z","iopub.status.idle":"2023-09-14T01:43:00.330828Z","shell.execute_reply.started":"2023-09-14T01:43:00.284040Z","shell.execute_reply":"2023-09-14T01:43:00.329871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix_sentiment=np.zeros((len(word_index_sentiment) + 1, 100))\nfor word, i in word_index_sentiment.items():\n    embedding_vector=embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_sentiment[i]=embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:01.354760Z","iopub.execute_input":"2023-09-14T01:43:01.355466Z","iopub.status.idle":"2023-09-14T01:43:01.360957Z","shell.execute_reply.started":"2023-09-14T01:43:01.355432Z","shell.execute_reply":"2023-09-14T01:43:01.359963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_text.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:02.355232Z","iopub.execute_input":"2023-09-14T01:43:02.356180Z","iopub.status.idle":"2023-09-14T01:43:02.362796Z","shell.execute_reply.started":"2023-09-14T01:43:02.356138Z","shell.execute_reply":"2023-09-14T01:43:02.361876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_sentiment.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:03.226654Z","iopub.execute_input":"2023-09-14T01:43:03.227042Z","iopub.status.idle":"2023-09-14T01:43:03.233199Z","shell.execute_reply.started":"2023-09-14T01:43:03.227012Z","shell.execute_reply":"2023-09-14T01:43:03.232194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing Testing dataset**","metadata":{}},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:07.960663Z","iopub.execute_input":"2023-09-14T01:43:07.961056Z","iopub.status.idle":"2023-09-14T01:43:07.973410Z","shell.execute_reply.started":"2023-09-14T01:43:07.961024Z","shell.execute_reply":"2023-09-14T01:43:07.972479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying preprocessing as training data\n\ntest['text'] = test['text'].apply(remove_hyperlinks)\ntest['text'] = test['text'].apply(clear_text)\ntest['text'] = test['text'].apply(expand_contractions)\ntest['text'] = test['text'].apply(abbrev2_text)\n#test['text'] = test['text'].apply(spacy_lemmatizer)\ntest['text'] = test['text'].apply(spacing_punctuation)\n\nprint(test['text'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:43:50.629792Z","iopub.execute_input":"2023-09-14T01:43:50.630544Z","iopub.status.idle":"2023-09-14T01:43:51.062844Z","shell.execute_reply.started":"2023-09-14T01:43:50.630509Z","shell.execute_reply":"2023-09-14T01:43:51.061753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply padding on test data\n\ntest_text = test['text'].values\ntest_sentiment = test['sentiment'].values\n\ntest_text = tokenizer_text.texts_to_sequences(test_text)\ntest_text = pad_sequences(test_text, maxlen = max_text_len, padding = 'post')\n\ntest_sentiment = tokenizer_sentiment.texts_to_sequences(test_sentiment)\ntest_sentiment = pad_sequences(test_sentiment, maxlen = max_sentiment_len, padding = 'post')\n\n\nprint(test_text.shape, test_sentiment.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:44:19.720225Z","iopub.execute_input":"2023-09-14T01:44:19.720706Z","iopub.status.idle":"2023-09-14T01:44:19.990909Z","shell.execute_reply.started":"2023-09-14T01:44:19.720667Z","shell.execute_reply":"2023-09-14T01:44:19.989964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jaccard Calculation","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    \n    a = set(str1.lower().split()) \n    \n    b = set(str2.lower().split())\n    \n    c = a.intersection(b)\n    \n    return float(len(c)) / (len(a) + len(b) - len(c) + 0.0000000000008)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:44:22.876561Z","iopub.execute_input":"2023-09-14T01:44:22.876991Z","iopub.status.idle":"2023-09-14T01:44:22.882703Z","shell.execute_reply.started":"2023-09-14T01:44:22.876959Z","shell.execute_reply":"2023-09-14T01:44:22.881747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Model","metadata":{}},{"cell_type":"markdown","source":"# GRU MODEL Training ","metadata":{}},{"cell_type":"code","source":"def build_model1(max_length_text, max_length_sentiment):\n    \n    input1=Input(shape=(max_length_text,),name='text_input')\n    \n    embed = Embedding(vocab_size_text,100,input_length=max_length_text,name='embedding_text',\\\n                          trainable=False,embeddings_initializer=tf.constant_initializer(embedding_matrix_text))(input1)\n    \n    gru = GRU(32,name='gru',return_sequences=True)(embed)\n    \n    f1=Flatten()(gru)\n    \n    input2=Input(shape=(max_length_sentiment,),name='sentiment_input')\n    \n    embed2=Embedding(vocab_size_sentiment,100,input_length=max_length_sentiment,name='embedding_sentiment')(input2)\n    \n    f2=Flatten()(embed2)\n    \n    concat1=Concatenate(axis=1)([f1,f2])\n    \n    dense1=Dense(16,activation='relu',kernel_regularizer=l2(0.0001))(concat1)\n    \n    drop1 = Dropout(0.6)(dense1)\n    \n    ln= LayerNormalization()(drop1)\n    \n    dense2=Dense(8,activation='relu',kernel_regularizer=l2(0.0001))(ln)\n    \n    output=Dense(2,name='output')(dense2)\n    \n    model = Model(inputs=[input1, input2], outputs=[output])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1 = build_model1(max_text_len, max_sentiment_len)\nmodel_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.compile(\n  optimizer= 'adam',\n  loss= 'mse',\n  metrics= ['accuracy']\n)\nplot_model(model_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_1.fit(x = (train_text, train_sentiment), y = train_label_data.values, batch_size = 128, epochs = 150, validation_data = val_data, validation_batch_size = 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bi LSTM model Training ","metadata":{}},{"cell_type":"code","source":"def build_model2(max_length_text, max_length_sentiment):\n    \n    input1 = Input(shape = (max_length_text, ), name = 'text_input')\n    \n    embed = Embedding(vocab_size_text, 100, input_length = max_length_text,name = 'embedding_text',\\\n                          trainable = False, embeddings_initializer = tf.constant_initializer(embedding_matrix_text))(input1)\n    \n    lstm = Bidirectional(LSTM(32, name = 'BLSTM', return_sequences = True))(embed)\n    \n    f1 = Flatten()(lstm)\n    \n    input2 = Input(shape = (max_length_sentiment, ), name = 'sentiment_input')\n    \n    embed2 = Embedding(vocab_size_sentiment, 100, input_length = max_length_sentiment, name = 'embedding_sentiment')(input2)\n    \n    f2 = Flatten()(embed2)\n    \n    concat1 = Concatenate(axis = 1)([f1, f2])\n    \n    dense1 = Dense(16, activation = 'relu',kernel_regularizer = l2(0.0001))(concat1)\n    \n    drop1 = Dropout(0.6)(dense1)\n    \n    ln = LayerNormalization()(drop1)\n    \n    dense2 = Dense(8, activation = 'relu', kernel_regularizer = l2(0.0001))(ln)\n    \n    output = Dense(2, name = 'output')(dense2)\n    \n    model = Model(inputs = [input1, input2], outputs = [output])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:46:16.752358Z","iopub.execute_input":"2023-09-14T01:46:16.752729Z","iopub.status.idle":"2023-09-14T01:46:16.763535Z","shell.execute_reply.started":"2023-09-14T01:46:16.752698Z","shell.execute_reply":"2023-09-14T01:46:16.762518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = build_model2(max_text_len, max_sentiment_len)\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:46:19.679515Z","iopub.execute_input":"2023-09-14T01:46:19.679876Z","iopub.status.idle":"2023-09-14T01:46:26.478202Z","shell.execute_reply.started":"2023-09-14T01:46:19.679847Z","shell.execute_reply":"2023-09-14T01:46:26.477390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.compile(\n  optimizer= 'adam',\n  loss= 'mse',\n  metrics= ['accuracy']\n)\nplot_model(model_2)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:46:28.032940Z","iopub.execute_input":"2023-09-14T01:46:28.033332Z","iopub.status.idle":"2023-09-14T01:46:28.537847Z","shell.execute_reply.started":"2023-09-14T01:46:28.033301Z","shell.execute_reply":"2023-09-14T01:46:28.536913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_2.fit(x = (train_text, train_sentiment), y = train_label_data.values, batch_size = 128, epochs = 150, validation_data = val_data, validation_batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:46:29.626811Z","iopub.execute_input":"2023-09-14T01:46:29.627972Z","iopub.status.idle":"2023-09-14T01:50:55.305366Z","shell.execute_reply.started":"2023-09-14T01:46:29.627924Z","shell.execute_reply":"2023-09-14T01:50:55.304232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train accuracy ', 'validation accuracy'], loc = 'lower right')\nplt.title(\"training accuracy and validation accuracy along epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:50:59.776480Z","iopub.execute_input":"2023-09-14T01:50:59.776857Z","iopub.status.idle":"2023-09-14T01:51:00.136762Z","shell.execute_reply.started":"2023-09-14T01:50:59.776824Z","shell.execute_reply":"2023-09-14T01:51:00.135801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\n\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train loss', 'validation loss'], loc='upper right')\nplt.title(\"training loss and validation loss along epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:51:01.875565Z","iopub.execute_input":"2023-09-14T01:51:01.876632Z","iopub.status.idle":"2023-09-14T01:51:02.183820Z","shell.execute_reply.started":"2023-09-14T01:51:01.876581Z","shell.execute_reply":"2023-09-14T01:51:02.182855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate Training Jaccard Score .","metadata":{}},{"cell_type":"code","source":"# load predictions of the training data to apply jaccard matric\n\ntraining_pred =  model_2.predict([train_text, train_sentiment])\nprint(len(training_pred))\nprint(training_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:51:07.444776Z","iopub.execute_input":"2023-09-14T01:51:07.445501Z","iopub.status.idle":"2023-09-14T01:51:10.795691Z","shell.execute_reply.started":"2023-09-14T01:51:07.445466Z","shell.execute_reply":"2023-09-14T01:51:10.794588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the output of the model from indices to text by extracting the substring from strat to end index\n\ndef find_sel_text(x):\n  txt , start , end = x[0], x[1], x[2]\n\n  end = end + 1\n  txt = txt.split()\n  sel_text = txt[start:end]\n  sel_text = \" \".join(sel_text)\n  return sel_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:53:29.372575Z","iopub.execute_input":"2023-09-14T01:53:29.373014Z","iopub.status.idle":"2023-09-14T01:53:29.379868Z","shell.execute_reply.started":"2023-09-14T01:53:29.372980Z","shell.execute_reply":"2023-09-14T01:53:29.378468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# by rounding the model output to int and sort it we get array that contain ythe start and end indices .\n\ntraining_pred = np.sort(abs((np.round(training_pred))))\n\ntrain_text_data['predicted_start'],train_text_data['predicted_end'] = (abs(training_pred[:,0])),(abs(training_pred[:,1]))\n\ntrain_text_data['predicted_start'] = train_text_data['predicted_start'].astype('int')\ntrain_text_data['predicted_end'] = train_text_data['predicted_end'].astype('int')\n\ntrain_text_data['predicted_selected_text'] = train_text_data[['text','predicted_start', 'predicted_end']].apply(lambda x:find_sel_text(x),axis=1)\ntrain_text_data['true_start'], train_text_data['true_end'] = train_label_data['start_indices'],train_label_data['end_indices'] \ntrain_text_data['true_selected_text'] = train_text_data[['text','true_start', 'true_end']].apply(lambda x:find_sel_text(x),axis=1)\n\ntrain_text_data['jaccard'] = train_text_data.apply(lambda y:jaccard(y.predicted_selected_text, y.true_selected_text), axis = 1)\n\nprint(\"\\n training total jaccard score\",np.mean(train_text_data['jaccard']), \"\\n\")\n\nprint(\"\\n training total jaccard score for positive samples\",np.mean(train_text_data[train_text_data['sentiment'] == 'positive']['jaccard']), \"\\n\")\n\nprint(\"\\n training total jaccard score for negative samples\",np.mean(train_text_data[train_text_data['sentiment'] == 'negative']['jaccard']), \"\\n\")\n\nprint(\"\\n training total jaccard score for neutral samples\",np.mean(train_text_data[train_text_data['sentiment'] == 'neutral']['jaccard']), \"\\n\")\n\ndisplay(train_text_data[:20])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:54:45.661130Z","iopub.execute_input":"2023-09-14T01:54:45.661513Z","iopub.status.idle":"2023-09-14T01:54:46.883519Z","shell.execute_reply.started":"2023-09-14T01:54:45.661483Z","shell.execute_reply":"2023-09-14T01:54:46.882470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle test dataset prediction","metadata":{}},{"cell_type":"code","source":"test_selected_indx  = model_2.predict([test_text,test_sentiment])\nprint(test_selected_indx.shape)\n\n\ntest_selected_indx = np.round(test_selected_indx)\n\n\n\ntest['start_indices'],test['end_indices'] = (abs(test_selected_indx[:,0])),(abs(test_selected_indx[:,1]))\n\ntest['start_indices'] = test['start_indices'].astype('int')\ntest['end_indices'] = test['end_indices'].astype('int')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:54:54.202764Z","iopub.execute_input":"2023-09-14T01:54:54.203517Z","iopub.status.idle":"2023-09-14T01:54:54.729502Z","shell.execute_reply.started":"2023-09-14T01:54:54.203481Z","shell.execute_reply":"2023-09-14T01:54:54.728585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['selected_text'] = test[['text','start_indices', 'end_indices']].apply(lambda x:find_sel_text(x),axis=1)\n\ntest","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:54:56.491772Z","iopub.execute_input":"2023-09-14T01:54:56.492477Z","iopub.status.idle":"2023-09-14T01:54:56.566085Z","shell.execute_reply.started":"2023-09-14T01:54:56.492442Z","shell.execute_reply":"2023-09-14T01:54:56.564910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Success & Failure Cases Analysis on analysis (test splitted from training) dataset","metadata":{}},{"cell_type":"code","source":"# apllying padding on analysis data (test data) to pass it to the model\n\nan_text = analysis_data['text'].values\nan_sentiment = analysis_data['sentiment'].values\n\n\nan_text = tokenizer_text.texts_to_sequences(an_text)\nan_text = pad_sequences(an_text, maxlen = max_text_len, padding = 'post')\n\nan_sentiment = tokenizer_sentiment.texts_to_sequences(an_sentiment)\nan_sentiment = pad_sequences(an_sentiment, maxlen = max_sentiment_len, padding = 'post')\n\n\nprint(an_text.shape, an_sentiment.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:55:42.761273Z","iopub.execute_input":"2023-09-14T01:55:42.761646Z","iopub.status.idle":"2023-09-14T01:55:42.949020Z","shell.execute_reply.started":"2023-09-14T01:55:42.761616Z","shell.execute_reply":"2023-09-14T01:55:42.948038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the pridected indices array\n\nanalysis_selected_indx  = model_2.predict([an_text,an_sentiment])\nprint(analysis_selected_indx.shape)\n\n\nanalysis_selected_indx = abs(np.round(analysis_selected_indx)).astype('int')\nanalysis_selected_indx = np.sort(analysis_selected_indx)\nprint((analysis_selected_indx))\n\nan_label = analysis_label.values\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:56:15.785859Z","iopub.execute_input":"2023-09-14T01:56:15.787090Z","iopub.status.idle":"2023-09-14T01:56:17.117316Z","shell.execute_reply.started":"2023-09-14T01:56:15.787041Z","shell.execute_reply":"2023-09-14T01:56:17.115969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nanalysis_list = list(zip(analysis_data['text'], analysis_data['sentiment'], analysis_selected_indx[:, 0], analysis_selected_indx[:, 1], an_label[:, 0], an_label[:, 1]))\n\nanalysis_df = pd.DataFrame(analysis_list, columns = ['text', 'sentiment', 'predicted_start', 'predicted_end','true_start', 'true_end'])\n\nanalysis_df['predicted_selected_text'] = analysis_df[['text','predicted_start', 'predicted_end']].apply(lambda x:find_sel_text(x),axis=1)\n\nanalysis_df['true_selected_text'] = analysis_df[['text','true_start', 'true_end']].apply(lambda x:find_sel_text(x),axis=1)\n\nanalysis_df['jaccard'] = analysis_df.apply(lambda y:jaccard(y.predicted_selected_text, y.true_selected_text), axis = 1)\n\n#display(analysis_df)\n\nprint(\"\\n analysis data total jaccard score\",np.mean(analysis_df['jaccard']), \"\\n\")\n\nprint(\"\\n analysis data tion total jaccard score for positive samples\",np.mean(analysis_df[analysis_df['sentiment'] == 'positive']['jaccard']), \"\\n\")\n\nprint(\"\\n analysis data total jaccard score for negative samples\",np.mean(analysis_df[analysis_df['sentiment'] == 'negative']['jaccard']), \"\\n\")\n \nprint(\"\\n analysis data total jaccard score for neutral samples\",np.mean(analysis_df[analysis_df['sentiment'] == 'neutral']['jaccard']), \"\\n\")\n\ndisplay(analysis_df[:20])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:56:19.069491Z","iopub.execute_input":"2023-09-14T01:56:19.069853Z","iopub.status.idle":"2023-09-14T01:56:19.524322Z","shell.execute_reply.started":"2023-09-14T01:56:19.069824Z","shell.execute_reply":"2023-09-14T01:56:19.522886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to classify the predictions to success and failure cases\n\ndef isSubset(arr1, arr2):\n    if arr1[0] <= arr2[0] and arr1[1] >= arr2[1]:\n        return True\n    else:\n        return False\n    \ndef check_intersection(arr1, arr2):\n    if arr1[0] == arr2[0] or arr1[1] == arr2[1]:\n        return True\n    elif arr1[0] < arr2[0] and arr1[1] < arr2[1] and arr2[0] < arr1[1]:\n        return True\n    elif arr1[0] > arr2[0] and arr1[1] > arr2[1] and arr1[0] < arr2[1]:\n        return True\n    elif arr1[0] > arr2[0] and arr1[1] < arr2[1]:\n        return True\n    else:\n        return False","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:58:37.153206Z","iopub.execute_input":"2023-09-14T01:58:37.153607Z","iopub.status.idle":"2023-09-14T01:58:37.162381Z","shell.execute_reply.started":"2023-09-14T01:58:37.153576Z","shell.execute_reply":"2023-09-14T01:58:37.161077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"success_index = []\nfail_index = []\nfail_with_intersection = []\n\nfor i in range(len(analysis_selected_indx)):\n    if(np.array_equal(analysis_selected_indx[i], an_label[i])):\n      success_index.append(i)\n    elif(check_intersection(analysis_selected_indx[i], an_label[i]) or isSubset(analysis_selected_indx[i], an_label[i])):\n        fail_with_intersection.append(i)\n    else:\n      fail_index.append(i)\n    \nprint(\"\\n failed cases length >> \",len(fail_index))\nprint(\"\\n success cases length >> \",len(success_index))\nprint(\"\\n intersection cases length >> \",len(fail_with_intersection))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:58:39.731202Z","iopub.execute_input":"2023-09-14T01:58:39.731615Z","iopub.status.idle":"2023-09-14T01:58:39.801284Z","shell.execute_reply.started":"2023-09-14T01:58:39.731580Z","shell.execute_reply":"2023-09-14T01:58:39.800109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"an_txt = analysis_data['text'].values\nan_sen = analysis_data['sentiment'].values\n\nsuccess_text = []\nfailure_text = []\nsuccess_sentiment = []\nfailure_sentiment = []\nsuccess_indices = []\nfailure_indices = []\nsuccess_ground = []\nfailure_ground = []\nintr_fail_text = []\nintr_fail_sent = []\nintr_fail_ind = []\nintr_fail_ground = []\n\nfor k in range(len(success_index)):\n    success_text.append(an_txt[success_index[k]])\n    success_sentiment.append(an_sen[success_index[k]])\n    success_indices.append(analysis_selected_indx[success_index[k]]) \n    success_ground.append(an_label[success_index[k]])\n\nfor k in range(len(fail_index)):\n    failure_text.append(an_txt[fail_index[k]])\n    failure_sentiment.append(an_sen[fail_index[k]])\n    failure_indices.append(analysis_selected_indx[fail_index[k]]) \n    failure_ground.append(an_label[fail_index[k]])\n    \nfor k in range(len(fail_with_intersection)):\n    intr_fail_text.append(an_txt[fail_with_intersection[k]])\n    intr_fail_sent.append(an_sen[fail_with_intersection[k]])\n    intr_fail_ind.append(analysis_selected_indx[fail_with_intersection[k]]) \n    intr_fail_ground.append(an_label[fail_with_intersection[k]])\n    \nsuccess_tup = list(zip(success_text, success_sentiment, success_indices, success_ground))  \nfailure_tup = list(zip(failure_text, failure_sentiment, failure_indices, failure_ground))\nintr_fail_tup = list(zip(intr_fail_text, intr_fail_sent, intr_fail_ind, intr_fail_ground))\nsucc_df = pd.DataFrame(success_tup, columns = ['text', 'sentiment', 'predicted indices', 'ground truth indices'])\nfail_df = pd.DataFrame(failure_tup, columns = ['text', 'sentiment', 'predicted indices', 'ground truth indices'])\nintr_fail_df = pd.DataFrame(intr_fail_tup, columns = ['text', 'sentiment', 'predicted indices', 'ground truth indices'])\nprint(\"\\n\")\nprint(\"          ****************** Some Success Cases ***********************\")\ndisplay(succ_df.iloc[:20])\nprint(\"\\n\")\nprint(\"           ****************** Some Failure Cases with Intersection ***********************\")\ndisplay(intr_fail_df.iloc[:20])\nprint(\"\\n\")\nprint(\"           ****************** Some Failure Cases ***********************\")\ndisplay(fail_df.iloc[:20])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:58:41.215030Z","iopub.execute_input":"2023-09-14T01:58:41.215396Z","iopub.status.idle":"2023-09-14T01:58:41.313991Z","shell.execute_reply.started":"2023-09-14T01:58:41.215366Z","shell.execute_reply":"2023-09-14T01:58:41.312947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"succ = tokenizer_text.texts_to_sequences(success_text)\nfail = tokenizer_text.texts_to_sequences(failure_text)\nsucc_fail = tokenizer_text.texts_to_sequences(intr_fail_text)\n\nsuccess_len_list = [len(seq) for seq in succ]\nfailure_len_list = [len(seq) for seq in fail]\nsucc_fail_len = [len(seq) for seq in succ_fail]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:58:42.725886Z","iopub.execute_input":"2023-09-14T01:58:42.726280Z","iopub.status.idle":"2023-09-14T01:58:42.823849Z","shell.execute_reply.started":"2023-09-14T01:58:42.726251Z","shell.execute_reply":"2023-09-14T01:58:42.822925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analyzing the lengths of success cases\n\nunique_len, freq = np.unique(success_len_list, return_counts=True)\nprint(unique_len, \"\\n\")\nprint(freq, \"\\n\")\nfig, ax = plt.subplots(figsize =(10, 8))\nax.barh(unique_len, freq, color ='orange')\nax.invert_yaxis()\nplt.title('sentences length distribution for success cases samples')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:59:29.650227Z","iopub.execute_input":"2023-09-14T01:59:29.650615Z","iopub.status.idle":"2023-09-14T01:59:30.018522Z","shell.execute_reply.started":"2023-09-14T01:59:29.650585Z","shell.execute_reply":"2023-09-14T01:59:30.017390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analyzing the lengths of failure cases\n\nunique_len, freq = np.unique(failure_len_list, return_counts=True)\nprint(unique_len, \"\\n\")\nprint(freq, \"\\n\")\nfig, ax = plt.subplots(figsize =(10, 8))\nax.barh(unique_len, freq)\nax.invert_yaxis()\nplt.title('sentences length distribution for failure cases samples')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T01:59:44.118365Z","iopub.execute_input":"2023-09-14T01:59:44.118725Z","iopub.status.idle":"2023-09-14T01:59:44.454241Z","shell.execute_reply.started":"2023-09-14T01:59:44.118696Z","shell.execute_reply":"2023-09-14T01:59:44.453173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analyzing the lengths of intersection cases\n\nunique_len, freq = np.unique(succ_fail_len, return_counts=True)\nprint(unique_len, \"\\n\")\nprint(freq, \"\\n\")\nfig, ax = plt.subplots(figsize =(10, 8))\nax.barh(unique_len, freq, color ='green')\nax.invert_yaxis()\nplt.title('sentences length distribution for intersection cases samples')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:00:03.663546Z","iopub.execute_input":"2023-09-14T02:00:03.664034Z","iopub.status.idle":"2023-09-14T02:00:03.992847Z","shell.execute_reply.started":"2023-09-14T02:00:03.663988Z","shell.execute_reply":"2023-09-14T02:00:03.991831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nat_succ = [success_sentiment[index] for (index, item) in enumerate(success_sentiment) if item == \"neutral\"]\npos_succ = [success_sentiment[index] for (index, item) in enumerate(success_sentiment) if item == \"positive\"]\nneg_succ = [success_sentiment[index] for (index, item) in enumerate(success_sentiment) if item == \"negative\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:00:49.040787Z","iopub.execute_input":"2023-09-14T02:00:49.041723Z","iopub.status.idle":"2023-09-14T02:00:49.048451Z","shell.execute_reply.started":"2023-09-14T02:00:49.041676Z","shell.execute_reply":"2023-09-14T02:00:49.047374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nat_fail = [failure_sentiment[index] for (index, item) in enumerate(failure_sentiment) if item == \"neutral\"]\npos_fail = [failure_sentiment[index] for (index, item) in enumerate(failure_sentiment) if item == \"positive\"]\nneg_fail = [failure_sentiment[index] for (index, item) in enumerate(failure_sentiment) if item == \"negative\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:00:49.997399Z","iopub.execute_input":"2023-09-14T02:00:49.998165Z","iopub.status.idle":"2023-09-14T02:00:50.005301Z","shell.execute_reply.started":"2023-09-14T02:00:49.998124Z","shell.execute_reply":"2023-09-14T02:00:50.004108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suc_fail_neu = [intr_fail_sent[index] for (index, item) in enumerate(intr_fail_sent) if item == \"neutral\"]\nsuc_fail_pos = [intr_fail_sent[index] for (index, item) in enumerate(intr_fail_sent) if item == \"positive\"]\nsuc_fail_neg = [intr_fail_sent[index] for (index, item) in enumerate(intr_fail_sent) if item == \"negative\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:00:50.674344Z","iopub.execute_input":"2023-09-14T02:00:50.674683Z","iopub.status.idle":"2023-09-14T02:00:50.681758Z","shell.execute_reply.started":"2023-09-14T02:00:50.674653Z","shell.execute_reply":"2023-09-14T02:00:50.680801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot showing the percentage of each sentiment in success samples\n\nval = [len(nat_succ), len(pos_succ), len(neg_succ)]\nlab = [\"neutral samples\", \"positive samples\", \"negative samples\"]\nplt.pie(val, labels = lab, autopct='%1.2f%%')\nplt.legend(loc = 'upper left')\nplt.title('success case')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:01:31.440373Z","iopub.execute_input":"2023-09-14T02:01:31.440777Z","iopub.status.idle":"2023-09-14T02:01:31.630640Z","shell.execute_reply.started":"2023-09-14T02:01:31.440745Z","shell.execute_reply":"2023-09-14T02:01:31.629689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot showing the percentage of each sentiment in failure samples\n\nval = [len(nat_fail), len(pos_fail), len(neg_fail)]\nlab = [\"neutral samples\", \"positive samples\", \"negative samples\"]\nplt.pie(val, labels = lab, autopct='%1.2f%%')\nplt.legend(loc = 'upper left')\nplt.title('failure case')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:01:45.769809Z","iopub.execute_input":"2023-09-14T02:01:45.770431Z","iopub.status.idle":"2023-09-14T02:01:45.987209Z","shell.execute_reply.started":"2023-09-14T02:01:45.770388Z","shell.execute_reply":"2023-09-14T02:01:45.985937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot showing the percentage of each sentiment in intersection samples\n\nval = [len(suc_fail_neu), len(suc_fail_pos), len(suc_fail_neg)]\nlab = [\"neutral samples\", \"positive samples\", \"negative samples\"]\nplt.pie(val, labels = lab, autopct='%1.2f%%')\nplt.legend(loc = 'upper left')\nplt.title('intersection case')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:01:59.122026Z","iopub.execute_input":"2023-09-14T02:01:59.122473Z","iopub.status.idle":"2023-09-14T02:01:59.347125Z","shell.execute_reply.started":"2023-09-14T02:01:59.122434Z","shell.execute_reply":"2023-09-14T02:01:59.346237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot show the achived jaccard scores and their frequencies\n\nsns.distplot(analysis_df['jaccard'], hist = True, kde = True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n\nplt.title('Density Plot of jaccard scores')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:03:36.437486Z","iopub.execute_input":"2023-09-14T02:03:36.437944Z","iopub.status.idle":"2023-09-14T02:03:36.854697Z","shell.execute_reply.started":"2023-09-14T02:03:36.437886Z","shell.execute_reply":"2023-09-14T02:03:36.853773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot show the achived jaccard scores for each sentiment and their frequencies\n\nsentiments = ['positive', 'negative', 'neutral']\n\nfor sentiment in sentiments:\n    # Subset to the airline\n    subset = analysis_df[analysis_df['sentiment'] == sentiment]\n    \n    # Draw the density plot\n    sns.distplot(subset['jaccard'], hist = False, kde = True,\n                 kde_kws = {'shade': True, 'linewidth': 3},\n                 label = sentiment)\n    \nplt.legend(prop={'size': 16}, title = 'sentiment')\nplt.title('Density Plot of jaccard scores for each sentimnet')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T02:03:37.928842Z","iopub.execute_input":"2023-09-14T02:03:37.929561Z","iopub.status.idle":"2023-09-14T02:03:38.704716Z","shell.execute_reply.started":"2023-09-14T02:03:37.929509Z","shell.execute_reply":"2023-09-14T02:03:38.703740Z"},"trusted":true},"execution_count":null,"outputs":[]}]}