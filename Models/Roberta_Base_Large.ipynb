{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-08T19:15:41.937008Z","iopub.execute_input":"2023-09-08T19:15:41.938092Z","iopub.status.idle":"2023-09-08T19:15:42.002216Z","shell.execute_reply.started":"2023-09-08T19:15:41.938052Z","shell.execute_reply":"2023-09-08T19:15:42.001162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions\n!pip install pyspellchecker\n!pip install -U textblob","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:15:58.360512Z","iopub.execute_input":"2023-09-08T19:15:58.361679Z","iopub.status.idle":"2023-09-08T19:16:40.489503Z","shell.execute_reply.started":"2023-09-08T19:15:58.361630Z","shell.execute_reply":"2023-09-08T19:16:40.488186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m textblob.download_corpora","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:16:43.634663Z","iopub.execute_input":"2023-09-08T19:16:43.635069Z","iopub.status.idle":"2023-09-08T19:16:46.617339Z","shell.execute_reply.started":"2023-09-08T19:16:43.635034Z","shell.execute_reply":"2023-09-08T19:16:46.615887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:16:48.757076Z","iopub.execute_input":"2023-09-08T19:16:48.757487Z","iopub.status.idle":"2023-09-08T19:17:01.221750Z","shell.execute_reply.started":"2023-09-08T19:16:48.757448Z","shell.execute_reply":"2023-09-08T19:17:01.220450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:17:16.104153Z","iopub.execute_input":"2023-09-08T19:17:16.104588Z","iopub.status.idle":"2023-09-08T19:17:17.455861Z","shell.execute_reply.started":"2023-09-08T19:17:16.104541Z","shell.execute_reply":"2023-09-08T19:17:17.454524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seaborn\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk \nimport string\nimport contractions\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\ntqdm_notebook.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:17:19.276401Z","iopub.execute_input":"2023-09-08T19:17:19.277843Z","iopub.status.idle":"2023-09-08T19:17:43.098431Z","shell.execute_reply.started":"2023-09-08T19:17:19.277780Z","shell.execute_reply":"2023-09-08T19:17:43.097345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Roberta Base","metadata":{}},{"cell_type":"code","source":"# load the data to a pandas dataframe\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_data, test_data = train_test_split(train, test_size = 0.2)\ntrain_data, valid_data = train_test_split(train_data, test_size=0.05)\nabbreviations = pd.read_csv(\"/kaggle/input/chat-slang-abbreviations-acronyms/slang/slang.csv\")\nabrevtn_dict   = dict(zip(abbreviations.acronym, abbreviations.expansion))","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:30:46.172496Z","iopub.execute_input":"2023-09-02T02:30:46.172877Z","iopub.status.idle":"2023-09-02T02:30:46.318731Z","shell.execute_reply.started":"2023-09-02T02:30:46.172847Z","shell.execute_reply":"2023-09-02T02:30:46.317769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2023-09-01T23:14:12.235244Z","iopub.execute_input":"2023-09-01T23:14:12.235630Z","iopub.status.idle":"2023-09-01T23:14:12.248959Z","shell.execute_reply.started":"2023-09-01T23:14:12.235597Z","shell.execute_reply":"2023-09-01T23:14:12.248007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2023-09-01T23:14:13.735112Z","iopub.execute_input":"2023-09-01T23:14:13.735467Z","iopub.status.idle":"2023-09-01T23:14:13.748755Z","shell.execute_reply.started":"2023-09-01T23:14:13.735439Z","shell.execute_reply":"2023-09-01T23:14:13.747819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data","metadata":{"execution":{"iopub.status.busy":"2023-09-01T23:14:15.655225Z","iopub.execute_input":"2023-09-01T23:14:15.655610Z","iopub.status.idle":"2023-09-01T23:14:15.668948Z","shell.execute_reply.started":"2023-09-01T23:14:15.655572Z","shell.execute_reply":"2023-09-01T23:14:15.667946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U watermark     \n!pip install seaborn\n!pip install -qq transformers\n\n%reload_ext watermark\n%watermark -v -p numpy,pandas,torch,transformers\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:17:48.721748Z","iopub.execute_input":"2023-09-08T19:17:48.722681Z","iopub.status.idle":"2023-09-08T19:18:36.867805Z","shell.execute_reply.started":"2023-09-08T19:17:48.722636Z","shell.execute_reply":"2023-09-08T19:18:36.866587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:04.225081Z","iopub.execute_input":"2023-09-08T19:19:04.225450Z","iopub.status.idle":"2023-09-08T19:19:04.231808Z","shell.execute_reply.started":"2023-09-08T19:19:04.225416Z","shell.execute_reply":"2023-09-08T19:19:04.230373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_text(text):\n  text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n  text=re.sub('`',\"'\",text)        #Replacing apostrophe\n  text=re.sub('\\S*\\d\\S*',' ',text) #Removing Numbers\n  text=re.sub('<.*?>+',' ',text)   #Removing Angular Brackets\n  text=re.sub('\\[.*?\\]',' ',text)  #Removing Square Brackets\n  text=re.sub('\\n',' ',text)       #Removing '\\n' character \n  text=re.sub('\\*+','INSULT',text) #Replacing **** by INSULT\n  return text","metadata":{"execution":{"iopub.status.busy":"2023-09-01T21:51:50.940001Z","iopub.execute_input":"2023-09-01T21:51:50.940391Z","iopub.status.idle":"2023-09-01T21:51:50.947007Z","shell.execute_reply.started":"2023-09-01T21:51:50.940342Z","shell.execute_reply":"2023-09-01T21:51:50.945970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_hyperlinks(text):\n  hyperlinkfree = re.sub('https?://\\S+|www\\.\\S+', '', text)\n  return hyperlinkfree","metadata":{"execution":{"iopub.status.busy":"2023-09-01T19:05:39.338696Z","iopub.execute_input":"2023-09-01T19:05:39.339930Z","iopub.status.idle":"2023-09-01T19:05:39.345254Z","shell.execute_reply.started":"2023-09-01T19:05:39.339888Z","shell.execute_reply":"2023-09-01T19:05:39.344267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_data['text'] = train_data['text'].apply(remove_hyperlinks)\ntrain_data['text'] = train_data['text'].apply(clear_text)\n\nvalid_data['text'] = valid_data['text'].apply(remove_hyperlinks)\nvalid_data['text'] = valid_data['text'].apply(clear_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T19:25:21.232891Z","iopub.execute_input":"2023-09-01T19:25:21.233309Z","iopub.status.idle":"2023-09-01T19:25:22.148846Z","shell.execute_reply.started":"2023-09-01T19:25:21.233270Z","shell.execute_reply":"2023-09-01T19:25:22.147822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dropna(axis = 0,inplace=True)\ntest_data.dropna(axis = 0,inplace=True)\nvalid_data.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:31:35.766303Z","iopub.execute_input":"2023-09-02T02:31:35.766657Z","iopub.status.idle":"2023-09-02T02:31:35.819098Z","shell.execute_reply.started":"2023-09-02T02:31:35.766627Z","shell.execute_reply":"2023-09-02T02:31:35.818204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:08.283979Z","iopub.execute_input":"2023-09-08T19:19:08.284615Z","iopub.status.idle":"2023-09-08T19:19:08.295417Z","shell.execute_reply.started":"2023-09-08T19:19:08.284572Z","shell.execute_reply":"2023-09-08T19:19:08.294308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_data['text'] = train_data['text'].astype(str)\ntrain_data['selected_text'] = train_data['selected_text'].astype(str)\n\nvalid_data['text'] = valid_data['text'].astype(str)\nvalid_data['selected_text'] = valid_data['selected_text'].astype(str)\n\ntest_data['text'] = test_data['text'].astype(str)\ntrain_data = train_data.reset_index()\nvalid_data = valid_data.reset_index()\ntest_data = test_data.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:31:43.720267Z","iopub.execute_input":"2023-09-02T02:31:43.720676Z","iopub.status.idle":"2023-09-02T02:31:43.745696Z","shell.execute_reply.started":"2023-09-02T02:31:43.720644Z","shell.execute_reply":"2023-09-02T02:31:43.744610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.20)\n    loss = tf.reduce_mean(loss)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:10.617908Z","iopub.execute_input":"2023-09-08T19:19:10.618319Z","iopub.status.idle":"2023-09-08T19:19:10.624425Z","shell.execute_reply.started":"2023-09-08T19:19:10.618287Z","shell.execute_reply":"2023-09-08T19:19:10.623040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.layers import Input,Softmax,Dense,Activation,Dropout\nfrom transformers import *\nimport tokenizers","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:31:47.913196Z","iopub.execute_input":"2023-09-02T02:31:47.913551Z","iopub.status.idle":"2023-09-02T02:33:29.656746Z","shell.execute_reply.started":"2023-09-02T02:31:47.913522Z","shell.execute_reply":"2023-09-02T02:33:29.655749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explanation\n\nThis code snippet serves a critical role in data preprocessing for sentiment analysis, potentially leveraging the RoBERTa model architecture. It begins by initializing a tokenizer tailored for RoBERTa, which is fundamental for encoding text data effectively. Sentiment labels are mapped to their corresponding token IDs to facilitate sentiment analysis.\n\nThe training data is reset, and NumPy arrays are prepared to store tokenized input, attention masks, and position masks. These arrays are crucial for shaping the input data for model training.\n\nThe code then enters a loop, processing each training example. During this loop, it tokenizes and encodes the text and selected text, determining their positions in the encoded sequences. The input sequences are constructed, incorporating special tokens and sentiment information.\n\nAdditionally, binary masks are generated to identify the start and end positions of the selected text within the sequence. Overall, this code segment is indispensable for NLP tasks, ensuring that data is properly prepared for training sentiment analysis models.\n","metadata":{}},{"cell_type":"code","source":"# Define the maximum sequence length for tokenization\nmax_len = 128\n\n# Initialize a ByteLevelBPETokenizer with RoBERTa vocabulary and merges\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab='/kaggle/input/tf-roberta/vocab-roberta-base.json',\n            merges='/kaggle/input/tf-roberta/merges-roberta-base.txt',\n            lowercase=True,\n            add_prefix_space=True\n)\n\n# Define sentiment IDs for 'positive', 'negative', and 'neutral'\nsentiment_id = {'positive': tokenizer.encode('positive').ids[0],\n                'negative': tokenizer.encode('negative').ids[0],\n                'neutral': tokenizer.encode('neutral').ids[0]}\n\n# Reset the index of the training data\ntrain_data.reset_index(inplace=True)\n\n# Prepare input data for training\ntot_tw = train_data.shape[0]\n\n# Initialize arrays to store tokenized input data, attention masks, and masks for start and end positions\ninput_ids = np.ones((tot_tw, max_len), dtype='int32')\nattention_mask = np.zeros((tot_tw, max_len), dtype='int32')\ntoken_type_ids = np.zeros((tot_tw, max_len), dtype='int32')\nstart_mask = np.zeros((tot_tw, max_len), dtype='int32')\nend_mask = np.zeros((tot_tw, max_len), dtype='int32')\n\n# Loop through each training example\nfor i in range(tot_tw):\n    # Preprocess the text and selected text\n    set1 = \" \"+\" \".join(train_data.loc[i,'text'].split())\n    set2 = \" \".join(train_data.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)] = 1\n    if set1[idx-1] == \" \":\n        set2_loc[idx-1] = 1\n  \n    # Tokenize and encode the text\n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx = []\n    # Find tokens that correspond to selected text\n    for k, (a, b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    # Get the sentiment token\n    senti_token = sentiment_id[train_data.loc[i,'sentiment']]\n    \n    # Construct input sequence and update attention mask\n    input_ids[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [senti_token] + [2] \n    attention_mask[i, :len(enc_set1.ids) + 5] = 1\n\n    # Update start and end masks if selected text tokens exist\n    if len(selected_text_token_idx) > 0:\n        start_mask[i, selected_text_token_idx[0] + 1] = 1\n        end_mask[i, selected_text_token_idx[-1] + 1] = 1\n\n# The code above processes and prepares the input data for training an NLP model for sentiment analysis.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:33:35.276202Z","iopub.execute_input":"2023-09-02T02:33:35.277030Z","iopub.status.idle":"2023-09-02T02:33:41.553221Z","shell.execute_reply.started":"2023-09-02T02:33:35.276995Z","shell.execute_reply":"2023-09-02T02:33:41.551550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation Data Preprocessing\n\nThis section of code follows a similar pattern to the previous one but is tailored for preparing the validation data for a sentiment analysis model. The validation data is reset for consistency.\n\nArrays are initialized to store tokenized validation input, attention masks, and position masks, mirroring the structure used for training data.\n\nWithin a loop iterating through each validation example, text and selected text are processed and tokenized. The position of selected text within the text is determined, and the encoding process is applied.\n\nJust like in the training data preprocessing, the code constructs input sequences by adding special tokens and sentiment information. Additionally, it generates start and end masks to identify the positions of selected text within the sequence.\n\nThis code is crucial for ensuring the validation data is appropriately formatted and ready for evaluation during the model training process.\n","metadata":{}},{"cell_type":"code","source":"# Reset the index of the validation data\nvalid_data.reset_index(inplace=True)\n\n# Prepare input data for validation\ntot_tw_val = valid_data.shape[0]\n\n# Initialize arrays to store tokenized validation input data, attention masks, and masks for start and end positions\ninput_ids_val = np.ones((tot_tw_val, max_len), dtype='int32')\nattention_mask_val = np.zeros((tot_tw_val, max_len), dtype='int32')\ntoken_type_ids_val = np.zeros((tot_tw_val, max_len), dtype='int32')\nstart_mask_val = np.zeros((tot_tw_val, max_len), dtype='int32')\nend_mask_val = np.zeros((tot_tw_val, max_len), dtype='int32')\n\n# Loop through each validation example\nfor i in range(tot_tw_val):\n    # Preprocess the text and selected text in the validation data\n    set1 = \" \"+\" \".join(valid_data.loc[i,'text'].split())\n    set2 = \" \".join(valid_data.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)] = 1\n    if set1[idx-1] == \" \":\n        set2_loc[idx-1] = 1\n  \n    # Tokenize and encode the text\n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx = []\n    # Find tokens that correspond to selected text\n    for k, (a, b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    # Get the sentiment token\n    senti_token = sentiment_id[valid_data.loc[i,'sentiment']]\n    \n    # Construct input sequence and update attention mask for validation data\n    input_ids_val[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [senti_token] + [2] \n    attention_mask_val[i, :len(enc_set1.ids) + 5] = 1\n\n    # Update start and end masks if selected text tokens exist\n    if len(selected_text_token_idx) > 0:\n        start_mask_val[i, selected_text_token_idx[0] + 1] = 1\n        end_mask_val[i, selected_text_token_idx[-1] + 1] = 1\n\n# The code above processes and prepares the input data for validation, similar to the training data preprocessing.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:33:45.061026Z","iopub.execute_input":"2023-09-02T02:33:45.061387Z","iopub.status.idle":"2023-09-02T02:33:45.372747Z","shell.execute_reply.started":"2023-09-02T02:33:45.061359Z","shell.execute_reply":"2023-09-02T02:33:45.371672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Data Preprocessing\n\nThis section of code is dedicated to preparing the test data for evaluation with a sentiment analysis model. It begins by resetting the index of the test data for consistency.\n\nArrays are initialized to store tokenized test input data, attention masks, and token type IDs, following a structure similar to the one used for training and validation data.\n\nWithin a loop that iterates through each test example, the code preprocesses the text data by adding spaces and tokenizes it. Sentiment information is obtained and used to construct the input sequence by adding special tokens.\n\nAdditionally, attention masks are generated to indicate which tokens should be attended to during evaluation.\n\nThe purpose of this code segment is to ensure that the test data is appropriately formatted and ready for use in evaluating the sentiment analysis model's performance.\n","metadata":{}},{"cell_type":"code","source":"# Reset the index of the test data\ntest_data.reset_index(inplace=True)\n\n# Get the total number of test examples\ntot_test_tw = test_data.shape[0]\n\n# Initialize arrays to store tokenized test input data, attention masks, and token type IDs\ninput_ids_t = np.ones((tot_test_tw, max_len), dtype='int32')\nattention_mask_t = np.zeros((tot_test_tw, max_len), dtype='int32')\ntoken_type_ids_t = np.zeros((tot_test_tw, max_len), dtype='int32')\n\n# Loop through each test example\nfor i in range(tot_test_tw):\n    # Preprocess the text in the test data\n    set1 = \" \" + \" \".join(test_data.loc[i, 'text'].split())\n    \n    # Tokenize and encode the text\n    enc_set1 = tokenizer.encode(set1)\n\n    # Get the sentiment token\n    s_token = sentiment_id[test_data.loc[i, 'sentiment']]\n    \n    # Construct input sequence and update attention mask for test data\n    input_ids_t[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [s_token] + [2]\n    attention_mask_t[i, :len(enc_set1.ids) + 5] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:33:48.057090Z","iopub.execute_input":"2023-09-02T02:33:48.057456Z","iopub.status.idle":"2023-09-02T02:33:48.724679Z","shell.execute_reply.started":"2023-09-02T02:33:48.057426Z","shell.execute_reply":"2023-09-02T02:33:48.723621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture Definition\n\nThis Python function defines the architecture of a sentiment analysis model. It takes token IDs, attention masks, and token type IDs as input.\n\nFirst, it loads a pre-trained RoBERTa model and its configuration. The input data is passed through RoBERTa to obtain contextual embeddings.\n\nTwo branches for sentiment prediction are created, one for start positions (`x1`) and one for end positions (`x2`). These branches consist of dropout layers, convolutional layers, activation functions, and dense layers.\n\n- Each branch starts with a dropout layer to prevent overfitting.\n- Convolutional layers with different filters and kernel sizes are used to capture features from the RoBERTa embeddings.\n- Leaky ReLU activation functions introduce non-linearity.\n- Dense layers with a single neuron are used for each branch.\n- Flatten layers prepare the output for the final activation function.\n\nThe model is then defined with the specified inputs and outputs, and it returns the sentiment predictions for both start and end positions.\n\nThis code defines the architecture for a sentiment analysis model, which can be trained and evaluated on the provided data.\n","metadata":{}},{"cell_type":"code","source":"def build_model():\n    # Define input layers for token IDs, attention masks, and token type IDs\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n\n    # Load the pre-trained RoBERTa model and its configuration\n    config_path = RobertaConfig.from_pretrained('/kaggle/input/tf-roberta/config-roberta-base.json')\n    roberta_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5', config=config_path)\n\n    # Pass input through the RoBERTa model\n    x = roberta_model(ids, attention_mask=att, token_type_ids=tok)\n\n    # Build the sentiment prediction layers for start and end positions\n    x1 = tf.keras.layers.Dropout(0.05)(x[0])\n    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.05)(x[0])\n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    # Define the model with inputs and outputs\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T17:31:07.940164Z","iopub.execute_input":"2023-09-08T17:31:07.940574Z","iopub.status.idle":"2023-09-08T17:31:07.953049Z","shell.execute_reply.started":"2023-09-08T17:31:07.940542Z","shell.execute_reply":"2023-09-08T17:31:07.951634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta = build_model()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T17:31:10.376083Z","iopub.execute_input":"2023-09-08T17:31:10.377180Z","iopub.status.idle":"2023-09-08T17:31:20.204543Z","shell.execute_reply.started":"2023-09-08T17:31:10.377135Z","shell.execute_reply":"2023-09-08T17:31:20.203516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \nmodel_roberta.compile(loss=custom_loss, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T17:31:23.495169Z","iopub.execute_input":"2023-09-08T17:31:23.495580Z","iopub.status.idle":"2023-09-08T17:31:23.515885Z","shell.execute_reply.started":"2023-09-08T17:31:23.495546Z","shell.execute_reply":"2023-09-08T17:31:23.514962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T17:31:26.885052Z","iopub.execute_input":"2023-09-08T17:31:26.885430Z","iopub.status.idle":"2023-09-08T17:31:26.961404Z","shell.execute_reply.started":"2023-09-08T17:31:26.885397Z","shell.execute_reply":"2023-09-08T17:31:26.960647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model_roberta, 'Model.png',show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:17:04.805746Z","iopub.execute_input":"2023-09-02T05:17:04.806156Z","iopub.status.idle":"2023-09-02T05:17:05.182551Z","shell.execute_reply.started":"2023-09-02T05:17:04.806119Z","shell.execute_reply":"2023-09-02T05:17:05.181621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation and Dataset Creation\n\nIn this code, the input data for training, validation, and testing is organized and converted into TensorFlow datasets.\n\n- For training, `input_data` contains token IDs, attention masks, and token type IDs, while `output_data` contains start and end position masks. A TensorFlow dataset is created from these inputs and outputs, and it's shuffled and batched to facilitate training. The `buffer_size` controls the shuffling.\n\n- For validation, `input_data_val` and `output_data_val` are similarly organized and used to create a TensorFlow dataset. This dataset is not shuffled as it's typically used for evaluation.\n\n- For testing, `input_data_test` is prepared to hold token IDs, attention masks, and token type IDs for the test data.\n\nThese datasets are essential for training, validating, and testing the sentiment analysis model, and they ensure that the data is efficiently processed in batches during training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"# Prepare training input and output data\ninput_data = (input_ids, attention_mask, token_type_ids)\noutput_data = (start_mask, end_mask)\n\n# Create a TensorFlow dataset from the training data, shuffling and batching it\ntrain_dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).shuffle(buffer_size=1024).batch(32)\n\n# Prepare validation input and output data\ninput_data_val = (input_ids_val, attention_mask_val, token_type_ids_val)\noutput_data_val = (start_mask_val, end_mask_val)\n\n# Create a TensorFlow dataset from the validation data, batching it\nvalid_dataset = tf.data.Dataset.from_tensor_slices((input_data_val, output_data_val)).batch(32)\n\n# Prepare test input data\ninput_data_test = (input_ids_t, attention_mask_t, token_type_ids_t)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:34:22.214150Z","iopub.execute_input":"2023-09-02T02:34:22.214503Z","iopub.status.idle":"2023-09-02T02:34:22.270984Z","shell.execute_reply.started":"2023-09-02T02:34:22.214475Z","shell.execute_reply":"2023-09-02T02:34:22.269972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_history = model_roberta.fit(train_dataset, validation_data = valid_dataset, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T02:34:28.161165Z","iopub.execute_input":"2023-09-02T02:34:28.161869Z","iopub.status.idle":"2023-09-02T04:22:36.281748Z","shell.execute_reply.started":"2023-09-02T02:34:28.161821Z","shell.execute_reply":"2023-09-02T04:22:36.280760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_pred , end_pred = model_roberta.predict([input_ids_t, attention_mask_t, token_type_ids_t])\nstart_pred.shape,end_pred.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:23:09.638354Z","iopub.execute_input":"2023-09-02T04:23:09.638814Z","iopub.status.idle":"2023-09-02T04:23:43.024800Z","shell.execute_reply.started":"2023-09-02T04:23:09.638781Z","shell.execute_reply":"2023-09-02T04:23:43.023765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Predicted Selected Text\n\nThis code segment is responsible for generating predictions for the selected text in the test data based on the model's start and end position predictions.\n\n- An empty list named `all` is initialized to store the predicted selected text for each test example.\n\n- The code iterates through each test example, obtaining the positions of the start and end tokens with the highest predicted probabilities.\n\n- If the start position is greater than the end position, it implies that the model predicts no valid span within the text. In this case, the entire original text is considered as the predicted selected text.\n\n- If there's a valid span (start position is less than or equal to the end position), the code reconstructs the selected text by tokenizing the original text, selecting the corresponding tokens, and decoding them.\n\n- The predicted selected text is then appended to the `all` list.\n\n- Finally, the predicted selected text is added as a new column ('pred_selected_text') in the `test_data` DataFrame, making it available for evaluation and comparison with the ground truth selected text.\n\nThis code is essential for evaluating the model's performance in predicting selected text in the test dataset.\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store predicted selected text\nall = []\n\n# Loop through each test example\nfor k in range(input_ids_t.shape[0]):\n    # Find the start and end positions with the highest probabilities\n    a = np.argmax(start_pred[k,])\n    b = np.argmax(end_pred[k,])\n    \n    # Check if the start position is greater than the end position\n    if a > b:\n        # If so, the selected text is the entire original text\n        st = test_data.loc[k,'text'] \n    else:\n        # If not, reconstruct the selected text from tokenized input\n        text1 = \" \" + \" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    \n    # Append the predicted selected text to the list\n    all.append(st)\n\n# Add the predicted selected text to the test_data DataFrame\ntest_data['pred_selected_text'] = all\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:23:52.194615Z","iopub.execute_input":"2023-09-02T04:23:52.195647Z","iopub.status.idle":"2023-09-02T04:23:52.812369Z","shell.execute_reply.started":"2023-09-02T04:23:52.195611Z","shell.execute_reply":"2023-09-02T04:23:52.811382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:23:54.909874Z","iopub.execute_input":"2023-09-02T04:23:54.910242Z","iopub.status.idle":"2023-09-02T04:23:54.934734Z","shell.execute_reply.started":"2023-09-02T04:23:54.910213Z","shell.execute_reply":"2023-09-02T04:23:54.933760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores=[]\nfor i in tqdm(range(len(test_data))):\n    scores.append(jaccard(test_data['selected_text'][i],test_data['pred_selected_text'][i]))","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:24:08.730469Z","iopub.execute_input":"2023-09-02T04:24:08.730869Z","iopub.status.idle":"2023-09-02T04:24:08.882960Z","shell.execute_reply.started":"2023-09-02T04:24:08.730837Z","shell.execute_reply":"2023-09-02T04:24:08.881865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['jaccard_scores'] = scores  ","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:24:11.024283Z","iopub.execute_input":"2023-09-02T04:24:11.024657Z","iopub.status.idle":"2023-09-02T04:24:11.031198Z","shell.execute_reply.started":"2023-09-02T04:24:11.024621Z","shell.execute_reply":"2023-09-02T04:24:11.030073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:08:06.253190Z","iopub.execute_input":"2023-09-02T05:08:06.253564Z","iopub.status.idle":"2023-09-02T05:08:06.274677Z","shell.execute_reply.started":"2023-09-02T05:08:06.253532Z","shell.execute_reply":"2023-09-02T05:08:06.273661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean jaccard score for neutral data:',test_data[test_data.sentiment =='neutral']['jaccard_scores'].mean())\nprint('Mean jaccard score for positive data:',test_data[test_data.sentiment =='positive']['jaccard_scores'].mean())\nprint('Mean jaccard score for negative data:',test_data[test_data.sentiment =='negative']['jaccard_scores'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:24:15.371211Z","iopub.execute_input":"2023-09-02T04:24:15.371576Z","iopub.status.idle":"2023-09-02T04:24:15.387899Z","shell.execute_reply.started":"2023-09-02T04:24:15.371548Z","shell.execute_reply":"2023-09-02T04:24:15.386677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean jaccard score for all data:',test_data['jaccard_scores'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:24:17.360298Z","iopub.execute_input":"2023-09-02T04:24:17.360669Z","iopub.status.idle":"2023-09-02T04:24:17.367286Z","shell.execute_reply.started":"2023-09-02T04:24:17.360638Z","shell.execute_reply":"2023-09-02T04:24:17.365923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[test_data['jaccard_scores'] == 1]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:24:21.379643Z","iopub.execute_input":"2023-09-02T04:24:21.380332Z","iopub.status.idle":"2023-09-02T04:24:21.402755Z","shell.execute_reply.started":"2023-09-02T04:24:21.380297Z","shell.execute_reply":"2023-09-02T04:24:21.401818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[test_data['jaccard_scores'] != 1]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T01:29:47.485938Z","iopub.execute_input":"2023-09-02T01:29:47.486330Z","iopub.status.idle":"2023-09-02T01:29:47.506472Z","shell.execute_reply.started":"2023-09-02T01:29:47.486300Z","shell.execute_reply":"2023-09-02T01:29:47.505486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss','val_loss'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T04:33:28.042394Z","iopub.execute_input":"2023-09-02T04:33:28.042834Z","iopub.status.idle":"2023-09-02T04:33:28.336594Z","shell.execute_reply.started":"2023-09-02T04:33:28.042801Z","shell.execute_reply":"2023-09-02T04:33:28.335646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta.save_weights('/kaggle/working/roberta-base-1')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:00:52.043918Z","iopub.execute_input":"2023-09-02T05:00:52.044308Z","iopub.status.idle":"2023-09-02T05:00:58.518119Z","shell.execute_reply.started":"2023-09-02T05:00:52.044280Z","shell.execute_reply":"2023-09-02T05:00:58.516689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle \nwith open('/kaggle/working/roberta_hist', 'wb') as file_pi:\n    pickle.dump(model_history.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:02:09.686425Z","iopub.execute_input":"2023-09-02T05:02:09.686819Z","iopub.status.idle":"2023-09-02T05:02:09.692556Z","shell.execute_reply.started":"2023-09-02T05:02:09.686787Z","shell.execute_reply":"2023-09-02T05:02:09.691594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(test_data['jaccard_scores'], hist = True, kde = True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n\nplt.title('Density Plot of jaccard scores')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:03:19.573619Z","iopub.execute_input":"2023-09-02T05:03:19.574341Z","iopub.status.idle":"2023-09-02T05:03:19.990181Z","shell.execute_reply.started":"2023-09-02T05:03:19.574305Z","shell.execute_reply":"2023-09-02T05:03:19.988945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsentiments = ['positive', 'negative', 'neutral']\n\nfor sentiment in sentiments:\n    # Subset to the airline\n    subset = test_data[test_data['sentiment'] == sentiment]\n    \n    # Draw the density plot\n    sns.distplot(subset[\"jaccard_scores\"], hist=False, kde=True,\n                 kde_kws={'shade': True, 'linewidth': 3},\n                 label=sentiment)\n\nplt.legend(prop={'size': 16}, title='sentiment')\nplt.title('Density Plot of jaccard scores for each sentiment')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T05:05:04.475130Z","iopub.execute_input":"2023-09-02T05:05:04.475535Z","iopub.status.idle":"2023-09-02T05:05:04.899728Z","shell.execute_reply.started":"2023-09-02T05:05:04.475504Z","shell.execute_reply":"2023-09-02T05:05:04.898788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Roberta Large","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_data, test_data = train_test_split(train, test_size = 0.2)\ntrain_data, valid_data = train_test_split(train_data, test_size=0.05)\nabbreviations = pd.read_csv(\"/kaggle/input/chat-slang-abbreviations-acronyms/slang/slang.csv\")\nabrevtn_dict   = dict(zip(abbreviations.acronym, abbreviations.expansion))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:19.501272Z","iopub.execute_input":"2023-09-08T19:19:19.501706Z","iopub.status.idle":"2023-09-08T19:19:19.670162Z","shell.execute_reply.started":"2023-09-08T19:19:19.501672Z","shell.execute_reply":"2023-09-08T19:19:19.669048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dropna(axis = 0,inplace=True)\ntest_data.dropna(axis = 0,inplace=True)\nvalid_data.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:21.853395Z","iopub.execute_input":"2023-09-08T19:19:21.854433Z","iopub.status.idle":"2023-09-08T19:19:21.907925Z","shell.execute_reply.started":"2023-09-08T19:19:21.854395Z","shell.execute_reply":"2023-09-08T19:19:21.906807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['text'] = train_data['text'].astype(str)\ntrain_data['selected_text'] = train_data['selected_text'].astype(str)\n\nvalid_data['text'] = valid_data['text'].astype(str)\nvalid_data['selected_text'] = valid_data['selected_text'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:23.834446Z","iopub.execute_input":"2023-09-08T19:19:23.835499Z","iopub.status.idle":"2023-09-08T19:19:23.850878Z","shell.execute_reply.started":"2023-09-08T19:19:23.835463Z","shell.execute_reply":"2023-09-08T19:19:23.849410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig\n#model = TFRobertaModel.from_pretrained('roberta-large')\nimport tokenizers","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:26.573537Z","iopub.execute_input":"2023-09-08T19:19:26.573912Z","iopub.status.idle":"2023-09-08T19:19:26.656438Z","shell.execute_reply.started":"2023-09-08T19:19:26.573882Z","shell.execute_reply":"2023-09-08T19:19:26.655367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Larger Tokenizer Data Preprocessing\n\nThis code segment involves using a larger tokenizer for data preprocessing, which may have a different vocabulary and configurations compared to the previous tokenizer.\n\n- The larger tokenizer is initialized with its own vocabulary and merges information.\n\n- Sentiment labels are mapped to their corresponding token IDs using the larger tokenizer.\n\n- The training data index is reset for consistency.\n\n- Input data arrays for tokenized input, attention masks, and position masks are initialized.\n\n- The code proceeds to preprocess each training example using the larger tokenizer. It tokenizes and encodes the text and selected text, determining their positions in the encoded sequences.\n\n- Sentiment tokens are obtained using the larger tokenizer.\n\n- Input sequences are constructed with special tokens and sentiment information, and attention masks are updated accordingly.\n\n- Start and end masks are generated if selected text tokens exist.\n\nThis code segment prepares the input data for training, taking into account the specifics of the larger tokenizer's vocabulary and configuration.\n","metadata":{}},{"cell_type":"code","source":"# Define the maximum sequence length for tokenization\nmax_len = 128\n\n# Initialize a larger ByteLevelBPETokenizer with a different vocabulary\ntokenizer_large = tokenizers.ByteLevelBPETokenizer(\n            vocab='/kaggle/input/robertalarge/vocab.json',\n            merges='/kaggle/input/robertalarge/merges.txt',\n            lowercase=True,\n            add_prefix_space=True\n)\n\n# Define sentiment IDs for 'positive', 'negative', and 'neutral'\nsentiment_id_large = {'positive': tokenizer_large.encode(\"input_ids\").ids[0],\n                'negative': tokenizer_large.encode('negative').ids[0],\n                'neutral': tokenizer_large.encode('neutral').ids[0]}\n\n# Reset the index of the training data\ntrain_data.reset_index(inplace=True)\n\n# input data formatting for training\ntot_tw = train_data.shape[0]\n\n# Initialize arrays to store tokenized input data, attention masks, and position masks\ninput_ids_large = np.ones((tot_tw, max_len), dtype='int32')\nattention_mask_large = np.zeros((tot_tw, max_len), dtype='int32')\ntoken_type_ids_large = np.zeros((tot_tw, max_len), dtype='int32')\nstart_mask_large = np.zeros((tot_tw, max_len), dtype='int32')\nend_mask_large = np.zeros((tot_tw, max_len), dtype='int32')\n\n# Loop through each training example\nfor i in range(tot_tw):\n    # Preprocess the text and selected text\n    set1 = \" \" + \" \".join(train_data.loc[i, 'text'].split())\n    set2 = \" \".join(train_data.loc[i, 'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)] = 1\n    if set1[idx-1] == \" \":\n        set2_loc[idx-1] = 1\n  \n    # Tokenize and encode the text using the larger tokenizer\n    enc_set1 = tokenizer_large.encode(set1)\n\n    selected_text_token_idx = []\n    # Find tokens that correspond to selected text\n    for k, (a, b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    # Get the sentiment token using the larger tokenizer\n    senti_token = sentiment_id_large[train_data.loc[i, 'sentiment']]\n    input_ids_large[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [senti_token] + [2] \n    attention_mask_large[i, :len(enc_set1.ids) + 5] = 1\n\n    # Update start and end masks if selected text tokens exist\n    if len(selected_text_token_idx) > 0:\n        start_mask_large[i, selected_text_token_idx[0] + 1] = 1\n        end_mask_large[i, selected_text_token_idx[-1] + 1] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:28.995717Z","iopub.execute_input":"2023-09-08T19:19:28.996171Z","iopub.status.idle":"2023-09-08T19:19:35.678607Z","shell.execute_reply.started":"2023-09-08T19:19:28.996135Z","shell.execute_reply":"2023-09-08T19:19:35.677496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Larger Tokenizer Data Preparation for Validation\n\nThis code segment is responsible for preparing the validation data using the larger tokenizer. It ensures that the validation data is properly tokenized and formatted for model evaluation.\n\n- The validation data index is reset for consistency.\n\n- Arrays are initialized to store tokenized validation input data, attention masks, and position masks, similar to the training data.\n\n- The code iterates through each validation example, preprocessing the text and selected text in the same manner as for training data.\n\n- Tokenization and encoding are performed using the larger tokenizer, and the positions of selected text tokens are identified.\n\n- Sentiment tokens are obtained using the larger tokenizer.\n\n- Input sequences are constructed with special tokens and sentiment information, and attention masks are updated accordingly.\n\n- Start and end masks are generated if selected text tokens exist for each validation example.\n\nThis code ensures that the validation data is appropriately processed and formatted for evaluation with the model using the larger tokenizer.\n","metadata":{}},{"cell_type":"code","source":"# Reset the index of the validation data\nvalid_data.reset_index(inplace=True)\n\n# Get the total number of validation examples\ntot_tw_val = valid_data.shape[0]\n\n# Initialize arrays to store tokenized validation input data, attention masks, and position masks\ninput_ids_val_large = np.ones((tot_tw_val, max_len), dtype='int32')\nattention_mask_val_large = np.zeros((tot_tw_val, max_len), dtype='int32')\ntoken_type_ids_val_large = np.zeros((tot_tw_val, max_len), dtype='int32')\nstart_mask_val_large = np.zeros((tot_tw_val, max_len), dtype='int32')\nend_mask_val_large = np.zeros((tot_tw_val, max_len), dtype='int32')\n\n# Loop through each validation example\nfor i in range(tot_tw_val):\n    # Preprocess the text and selected text for validation\n    set1 = \" \" + \" \".join(valid_data.loc[i, 'text'].split())\n    set2 = \" \".join(valid_data.loc[i, 'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)] = 1\n    if set1[idx-1] == \" \":\n        set2_loc[idx-1] = 1\n  \n    # Tokenize and encode the text using the larger tokenizer\n    enc_set1 = tokenizer_large.encode(set1)\n\n    selected_text_token_idx = []\n    # Find tokens that correspond to selected text\n    for k, (a, b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    # Get the sentiment token using the larger tokenizer\n    senti_token = sentiment_id_large[valid_data.loc[i, 'sentiment']]\n    input_ids_val_large[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [senti_token] + [2] \n    attention_mask_val_large[i, :len(enc_set1.ids) + 5] = 1\n\n    # Update start and end masks if selected text tokens exist\n    if len(selected_text_token_idx) > 0:\n        start_mask_val_large[i, selected_text_token_idx[0] + 1] = 1\n        end_mask_val_large[i, selected_text_token_idx[-1] + 1] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:38.445711Z","iopub.execute_input":"2023-09-08T19:19:38.446343Z","iopub.status.idle":"2023-09-08T19:19:38.859188Z","shell.execute_reply.started":"2023-09-08T19:19:38.446297Z","shell.execute_reply":"2023-09-08T19:19:38.858022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Larger Tokenizer Data Preparation for Testing\n\nThis code segment prepares the test data for evaluation using the larger tokenizer. It ensures that the test data is properly tokenized and formatted for model predictions.\n\n- The test data index is reset for consistency.\n\n- Arrays are initialized to store tokenized test input data, attention masks, and token type IDs, similar to the training and validation data.\n\n- The code iterates through each test example, preprocessing the text in the same manner as for training and validation data.\n\n- Tokenization and encoding are performed using the larger tokenizer.\n\n- Sentiment tokens are obtained using the larger tokenizer.\n\n- Input sequences are constructed with special tokens and sentiment information, and attention masks are updated accordingly.\n\nThis code segment ensures that the test data is appropriately processed and formatted for making predictions with the model using the larger tokenizer.\n","metadata":{}},{"cell_type":"code","source":"# Reset the index of the test data\ntest_data.reset_index(inplace=True)\n\n# Get the total number of test examples\ntot_test_tw = test_data.shape[0]\n\n# Initialize arrays to store tokenized test input data, attention masks, and token type IDs\ninput_ids_t_large = np.ones((tot_test_tw, max_len), dtype='int32')\nattention_mask_t_large = np.zeros((tot_test_tw, max_len), dtype='int32')\ntoken_type_ids_t_large = np.zeros((tot_test_tw, max_len), dtype='int32')\n\n# Loop through each test example\nfor i in range(tot_test_tw):\n    # Preprocess the text for testing\n    set1 = \" \" + \" \".join(test_data.loc[i, 'text'].split())\n    \n    # Tokenize and encode the text using the larger tokenizer\n    enc_set1 = tokenizer_large.encode(set1)\n\n    # Get the sentiment token using the larger tokenizer\n    s_token = sentiment_id_large[test_data.loc[i, 'sentiment']]\n    \n    # Update input sequences and attention masks\n    input_ids_t_large[i, :len(enc_set1.ids) + 5] = [0] + enc_set1.ids + [2, 2] + [s_token] + [2]\n    attention_mask_t_large[i, :len(enc_set1.ids) + 5] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:41.080748Z","iopub.execute_input":"2023-09-08T19:19:41.081130Z","iopub.status.idle":"2023-09-08T19:19:41.735226Z","shell.execute_reply.started":"2023-09-08T19:19:41.081098Z","shell.execute_reply":"2023-09-08T19:19:41.734106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a Model with Larger Tokenizer\n\nThis code defines a function to build a neural model for text sentiment analysis using a larger tokenizer and pre-trained Roberta architecture.\n\n- The function takes three inputs: token IDs, attention masks, and token type IDs.\n\n- A configuration for the Roberta model is initialized.\n\n- The pre-trained Roberta model with the larger tokenizer ('roberta-large') is loaded.\n\n- Tokenized inputs are passed through the model, resulting in encoded representations.\n\n- Two sub-models are defined for predicting start and end positions. These sub-models consist of convolutional layers, dropout, activation functions, and dense layers.\n\n- The final model is created by specifying the input and output layers.\n\nThis code segment encapsulates the creation of a neural model suitable for sentiment analysis with a larger tokenizer, leveraging pre-trained representations from 'roberta-large'.\n","metadata":{}},{"cell_type":"code","source":"# Define a function to build a model using the larger tokenizer\ndef build_model_large():\n    # Define input layers for token IDs, attention masks, and token type IDs\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok =  tf.keras.layers.Input((max_len,), dtype=tf.int32) \n\n    # Initialize a configuration for the Roberta model\n    config_path = RobertaConfig()\n\n    # Load the pre-trained Roberta model with the larger tokenizer\n    roberta_model_large = TFRobertaModel.from_pretrained('roberta-large')\n\n    # Pass inputs through the Roberta model\n    x = roberta_model_large(ids, attention_mask=att, token_type_ids=tok)\n\n    # Define the model architecture for start position prediction\n    x1 = tf.keras.layers.Dropout(0.05)(x[0])\n    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)  # 128 filters; 2 is the kernel size of each filter\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    # Define the model architecture for end position prediction\n    x2 = tf.keras.layers.Dropout(0.05)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    # Create the model with inputs and outputs\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:44.385306Z","iopub.execute_input":"2023-09-08T19:19:44.385740Z","iopub.status.idle":"2023-09-08T19:19:44.400241Z","shell.execute_reply.started":"2023-09-08T19:19:44.385706Z","shell.execute_reply":"2023-09-08T19:19:44.398778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta_large = build_model_large()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:19:49.038646Z","iopub.execute_input":"2023-09-08T19:19:49.039032Z","iopub.status.idle":"2023-09-08T19:20:19.855043Z","shell.execute_reply.started":"2023-09-08T19:19:49.039002Z","shell.execute_reply":"2023-09-08T19:20:19.853922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer_large = tf.keras.optimizers.Adam(learning_rate=3e-5) \nmodel_roberta_large.compile(loss=custom_loss, optimizer=optimizer_large)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:20:24.876881Z","iopub.execute_input":"2023-09-08T19:20:24.877601Z","iopub.status.idle":"2023-09-08T19:20:24.913061Z","shell.execute_reply.started":"2023-09-08T19:20:24.877558Z","shell.execute_reply":"2023-09-08T19:20:24.911939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta_large.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:20:28.687472Z","iopub.execute_input":"2023-09-08T19:20:28.688229Z","iopub.status.idle":"2023-09-08T19:20:28.793798Z","shell.execute_reply.started":"2023-09-08T19:20:28.688195Z","shell.execute_reply":"2023-09-08T19:20:28.792711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model_roberta_large, 'Model_large.png',show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:20:31.089171Z","iopub.execute_input":"2023-09-08T19:20:31.089569Z","iopub.status.idle":"2023-09-08T19:20:31.489484Z","shell.execute_reply.started":"2023-09-08T19:20:31.089513Z","shell.execute_reply":"2023-09-08T19:20:31.488463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation and Dataset Creation for Larger Tokenizer\n\nThis code segment prepares the data input and output for the model using the larger tokenizer and creates datasets for training, validation, and testing.\n\n- Input and output data are organized into tuples for training and validation with the larger tokenizer.\n\n- A training dataset is created by slicing the input and output data tensors, shuffling the data, and batching it into smaller groups (batch size 16 in this case) for efficient training.\n\n- Similar steps are followed to create a validation dataset from tensor slices, where shuffling is not necessary.\n\n- Input data for testing with the larger tokenizer is also defined as a tuple.\n\nThis code ensures that the data is appropriately formatted and organized for training, validation, and testing, facilitating the training and evaluation of the model with the larger tokenizer.\n","metadata":{}},{"cell_type":"code","source":"# Define input and output data for the larger tokenizer\ninput_data_large = (input_ids_large, attention_mask_large, token_type_ids_large)\noutput_data_large = (start_mask_large, end_mask_large)\n\n# Create a training dataset from tensor slices, shuffling the data and batching it\ntrain_dataset_large = tf.data.Dataset.from_tensor_slices((input_data_large, output_data_large)).shuffle(buffer_size=1024).batch(16)\n\n# Define input and output data for validation using the larger tokenizer\ninput_data_val_large = (input_ids_val_large, attention_mask_val_large, token_type_ids_val_large)\noutput_data_val_large = (start_mask_val_large, end_mask_val_large)\n\n# Create a validation dataset from tensor slices and batch it\nvalid_dataset_large = tf.data.Dataset.from_tensor_slices((input_data_val_large, output_data_val_large)).batch(16)\n\n# Define input data for testing with the larger tokenizer\ninput_data_test_large = (input_ids_t_large, attention_mask_t_large, token_type_ids_t_large)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:20:34.332917Z","iopub.execute_input":"2023-09-08T19:20:34.333326Z","iopub.status.idle":"2023-09-08T19:20:34.382952Z","shell.execute_reply.started":"2023-09-08T19:20:34.333294Z","shell.execute_reply":"2023-09-08T19:20:34.381764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_history_large = model_roberta_large.fit(train_dataset_large, validation_data = valid_dataset_large, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T19:20:39.969142Z","iopub.execute_input":"2023-09-08T19:20:39.969567Z","iopub.status.idle":"2023-09-09T01:36:00.926699Z","shell.execute_reply.started":"2023-09-08T19:20:39.969507Z","shell.execute_reply":"2023-09-09T01:36:00.925278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_pred , end_pred = model_roberta_large.predict([input_ids_t_large, attention_mask_t_large, token_type_ids_t_large])\nstart_pred.shape,end_pred.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:38:00.976617Z","iopub.execute_input":"2023-09-09T01:38:00.977368Z","iopub.status.idle":"2023-09-09T01:40:30.392644Z","shell.execute_reply.started":"2023-09-09T01:38:00.977328Z","shell.execute_reply":"2023-09-09T01:40:30.391466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Predictions with Larger Tokenizer\n\nThis code segment is responsible for generating predictions for the selected text using the larger tokenizer and the model's output.\n\n- An empty list is initialized to store the predicted selected text for each example.\n\n- The code iterates through each example in the test data and determines the predicted start and end positions by finding the indices of maximum values in the model's output.\n\n- If the predicted start position is greater than the end position, it implies that the model did not find a valid selected text span, so the entire text is used as the predicted selected text.\n\n- Otherwise, the code tokenizes and decodes the text using the larger tokenizer to obtain the predicted selected text.\n\n- The predicted selected text is appended to the list.\n\n- Finally, the predicted selected text is added as a new column to the test_data DataFrame.\n\nThis code is essential for generating predictions for the test data using the model and larger tokenizer, which can be further evaluated for accuracy.\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store predicted selected text\nall = []\n\n# Loop through each example in the test data\nfor k in range(input_ids_t_large.shape[0]):\n    # Find the indices of the predicted start and end positions\n    a = np.argmax(start_pred[k,])\n    b = np.argmax(end_pred[k,])\n    \n    # Determine the selected text based on predictions\n    if a > b: \n        # If the predicted start position is greater than the end position, use the entire text\n        st = test_data.loc[k, 'text'] \n    else:\n        # Tokenize and decode the text to get the selected text\n        text1 = \" \" + \" \".join(test_data.loc[k, 'text'].split())\n        enc = tokenizer_large.encode(text1)\n        st = tokenizer_large.decode(enc.ids[a-1:b])\n    \n    # Append the predicted selected text to the list\n    all.append(st)\n\n# Add the predicted selected text to the test_data DataFrame\ntest_data['pred_selected_text'] = all\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:42:38.156968Z","iopub.execute_input":"2023-09-09T01:42:38.157417Z","iopub.status.idle":"2023-09-09T01:42:38.911418Z","shell.execute_reply.started":"2023-09-09T01:42:38.157383Z","shell.execute_reply":"2023-09-09T01:42:38.910194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores=[]\nfor i in tqdm(range(len(test_data))):\n    scores.append(jaccard(test_data['selected_text'][i],test_data['pred_selected_text'][i]))","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:42:48.988576Z","iopub.execute_input":"2023-09-09T01:42:48.988976Z","iopub.status.idle":"2023-09-09T01:42:49.201294Z","shell.execute_reply.started":"2023-09-09T01:42:48.988943Z","shell.execute_reply":"2023-09-09T01:42:49.198891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['jaccard_scores'] = scores ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:42:52.014945Z","iopub.execute_input":"2023-09-09T01:42:52.015352Z","iopub.status.idle":"2023-09-09T01:42:52.024132Z","shell.execute_reply.started":"2023-09-09T01:42:52.015318Z","shell.execute_reply":"2023-09-09T01:42:52.022675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:42:54.738713Z","iopub.execute_input":"2023-09-09T01:42:54.739438Z","iopub.status.idle":"2023-09-09T01:42:54.770209Z","shell.execute_reply.started":"2023-09-09T01:42:54.739405Z","shell.execute_reply":"2023-09-09T01:42:54.769087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean jaccard score for neutral data:',test_data[test_data.sentiment =='neutral']['jaccard_scores'].mean())\nprint('Mean jaccard score for positive data:',test_data[test_data.sentiment =='positive']['jaccard_scores'].mean())\nprint('Mean jaccard score for negative data:',test_data[test_data.sentiment =='negative']['jaccard_scores'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:43:07.414684Z","iopub.execute_input":"2023-09-09T01:43:07.415098Z","iopub.status.idle":"2023-09-09T01:43:07.432019Z","shell.execute_reply.started":"2023-09-09T01:43:07.415049Z","shell.execute_reply":"2023-09-09T01:43:07.430707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean jaccard score for all data:',test_data['jaccard_scores'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:43:12.247608Z","iopub.execute_input":"2023-09-09T01:43:12.248050Z","iopub.status.idle":"2023-09-09T01:43:12.255769Z","shell.execute_reply.started":"2023-09-09T01:43:12.248015Z","shell.execute_reply":"2023-09-09T01:43:12.254274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(model_history_large.history['loss'])\nplt.plot(model_history_large.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss','val_loss'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:49:32.027339Z","iopub.execute_input":"2023-09-09T01:49:32.029872Z","iopub.status.idle":"2023-09-09T01:49:32.389680Z","shell.execute_reply.started":"2023-09-09T01:49:32.029823Z","shell.execute_reply":"2023-09-09T01:49:32.388540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_roberta_large.save_weights('/kaggle/working/roberta-large-1')","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:49:41.181645Z","iopub.execute_input":"2023-09-09T01:49:41.182032Z","iopub.status.idle":"2023-09-09T01:50:06.254597Z","shell.execute_reply.started":"2023-09-09T01:49:41.181999Z","shell.execute_reply":"2023-09-09T01:50:06.244494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle \nwith open('/kaggle/working/roberta_large_hist', 'wb') as file_pi:\n    pickle.dump(model_history_large.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:50:24.392782Z","iopub.execute_input":"2023-09-09T01:50:24.393986Z","iopub.status.idle":"2023-09-09T01:50:24.401675Z","shell.execute_reply.started":"2023-09-09T01:50:24.393940Z","shell.execute_reply":"2023-09-09T01:50:24.400544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(test_data['jaccard_scores'], hist = True, kde = True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n\nplt.title('Density Plot of jaccard scores')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:50:33.322994Z","iopub.execute_input":"2023-09-09T01:50:33.323382Z","iopub.status.idle":"2023-09-09T01:50:33.874623Z","shell.execute_reply.started":"2023-09-09T01:50:33.323352Z","shell.execute_reply":"2023-09-09T01:50:33.873511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsentiments = ['positive', 'negative', 'neutral']\n\nfor sentiment in sentiments:\n    # Subset to the airline\n    subset = test_data[test_data['sentiment'] == sentiment]\n    \n    # Draw the density plot\n    sns.distplot(subset[\"jaccard_scores\"], hist=False, kde=True,\n                 kde_kws={'shade': True, 'linewidth': 3},\n                 label=sentiment)\n\nplt.legend(prop={'size': 16}, title='sentiment')\nplt.title('Density Plot of jaccard scores for each sentiment')\nplt.xlabel('jaccard score')\nplt.ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T01:50:40.660729Z","iopub.execute_input":"2023-09-09T01:50:40.661666Z","iopub.status.idle":"2023-09-09T01:50:41.141597Z","shell.execute_reply.started":"2023-09-09T01:50:40.661630Z","shell.execute_reply":"2023-09-09T01:50:41.140492Z"},"trusted":true},"execution_count":null,"outputs":[]}]}